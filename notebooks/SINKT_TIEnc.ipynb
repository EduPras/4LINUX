{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71ac968",
   "metadata": {},
   "source": [
    "# TIEnc\n",
    "\n",
    "<div>\n",
    "\n",
    "<img src=\"imgs/new_rotated.jpg\" alt=\"Description\" style=\"display: block; margin: 20px auto; width: 80%;\" />\n",
    "\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Generator | Validator | Status | Nodes | Chapters | Concepts | Relations | Prereqs | Part-Of | Defs | Props | Orphans | AvgRel/Node |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| claude-opus-4-5 | gpt5-1 | ✅ OK | 450 | 104 | 346 | 1395 | 71 | 632 | 30 | 143 | 0 | 3.1 |\n",
    "| claude-opus-4-5 | claude-opus-4-5 | ✅ OK | 460 | 104 | 356 | 1432 | 69 | 600 | 26 | 198 | 0 | 3.11 |\n",
    "| gpt5-1 | gpt5-1 | ✅ OK | 459 | 104 | 355 | 1726 | 119 | 536 | 209 | 242 | 0 | 3.76 |\n",
    "| gpt5-1 | claude-opus-4-5 | ✅ OK | 423 | 104 | 319 | 1601 | 137 | 490 | 117 | 249 | 0 | 3.78 |\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c046f4",
   "metadata": {},
   "source": [
    "**Estruturas & Modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d39aa9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T23:55:23.499381227Z",
     "start_time": "2025-12-12T23:55:23.479133056Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from langchain.chat_models import init_chat_model, BaseChatModel\n",
    "\n",
    "class Models(str, Enum):\n",
    "    GPT4_o = \"openai:gpt-4o\"\n",
    "    GPT5_1 = \"openai:gpt-5.1\"\n",
    "    CLAUDE4_5 = \"anthropic:claude-opus-4-5\"\n",
    "\n",
    "def get_llm(model_name: Models) -> BaseChatModel:\n",
    "    return init_chat_model(model_name.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "094a164c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T22:38:08.702078334Z",
     "start_time": "2025-12-12T22:38:08.646348041Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "class Concept(BaseModel):\n",
    "    \"\"\"Represents a single educational concept found in the text.\"\"\"\n",
    "    concept_name: str = Field(description=\"The formal name of the concept (e.g., 'Inductive Logic', 'Backpropagation').\")\n",
    "    chapter: List[int] = Field(description=\"The number of the current chapter, subchapter, etc (e.g., [1] for chapter 1, [1, 2] for subchapter 1.2, [1,2,5] for subsubchapter 1.2.5)\")\n",
    "    description: str = Field(description=\"A concise definition or summary of the concept based on the text.\")\n",
    "    # page_start: int = Field(description=\"The page number where this concept is first introduced.\")\n",
    "    # page_end: Optional[int] = Field(default=None, description=\"The page number where the discussion of this concept seems to end (or current page if ongoing).\")\n",
    "    # is_main_chapter: bool = Field(default=False, description=\"True if this is a chapter or main topic, False if it is a subchapter or subtopic.\")\n",
    "\n",
    "class PageExtraction(BaseModel):\n",
    "    \"\"\"Container for multiple concepts found on a specific page processing step.\"\"\"\n",
    "    concepts: List[Concept] = Field(description=\"List of concepts extracted from the current text window.\")\n",
    "    \n",
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"The subject concept.\")\n",
    "    target: str = Field(description=\"The object concept.\")\n",
    "    # relation_type: Literal['prerequisite', 'including', 'part-of', 'property', 'definition']\n",
    "    relation_type: Literal['prerequisite']\n",
    "    context: Optional[str] = Field(description=\"Justification text.\")\n",
    "\n",
    "class Critique(BaseModel):\n",
    "    \"\"\"A critique of a specific relation.\"\"\"\n",
    "    source: str\n",
    "    target: str\n",
    "    is_valid: bool = Field(description=\"True if the relation is supported by text and logic, False otherwise.\")\n",
    "    reasoning: str = Field(description=\"Why this relation is valid, invalid, or redundant.\")\n",
    "\n",
    "class ModerationDecision(BaseModel):\n",
    "    \"\"\"The Moderator's final decision on a specific relation.\"\"\"\n",
    "    source: str\n",
    "    target: str\n",
    "    status: Literal['approved', 'rejected'] = Field(description=\"Final verdict.\")\n",
    "    comments: str = Field(description=\"Why the moderator made this decision.\")\n",
    "\n",
    "class ConceptList(BaseModel):\n",
    "    concepts: List[Concept]\n",
    "    \n",
    "class RelationList(BaseModel):\n",
    "    relations: List[Relation]\n",
    "\n",
    "class CritiqueList(BaseModel):\n",
    "    critiques: List[Critique]\n",
    "\n",
    "class ModerationList(BaseModel):\n",
    "    decisions: List[ModerationDecision]\n",
    "\n",
    "\n",
    "class ConceptAnalysis(BaseModel):\n",
    "    \"\"\"LLM Output for a full chapter/concept file.\"\"\"\n",
    "    # Map 'new_concepts' to add to registry\n",
    "    # new_concepts: List[str] = Field(description=\"List of MAIN concepts defined in this text.\")\n",
    "    relations: List[Relation] = Field(description=\"Semantic connections found.\")\n",
    "    \n",
    "class ValidationResult(BaseModel):\n",
    "    valid_relations: List[Relation] = Field(description=\"The filtered list of strictly valid educational relations.\")\n",
    "    rejected_relations: List[Relation] = Field(description=\"List of relations that were removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370987af",
   "metadata": {},
   "source": [
    "## Extração de dados\n",
    "\n",
    "O SINKT já considera um dataset pronto para uso. Sendo assim essa seção busca extrair os conceitos de um ebook PDF. Primeiramente iremos transformar em Markdown, visto que é melhor utilizar texto puro ao invés de páginas de PDF. Além disso, essa proposta facilita a própria extração para o MAIC, posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3c9ffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T23:57:18.313775597Z",
     "start_time": "2025-12-12T23:57:18.242271837Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "\n",
    "def normalize_filename(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove accents, replace underscores and remove non-alphanumeric characters.\n",
    "    \n",
    "    :param s: String to be normalized\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize('NFKD', s)\n",
    "    s = ''.join(char for char in s if not unicodedata.combining(char))\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w_]', '', s)\n",
    "    return s.lower()\n",
    "\n",
    "def prettify_xml(elem: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Return a pretty-printed XML string for the Element.\n",
    "    Strips the annoying extra newlines minidom likes to add.\n",
    "    \n",
    "    :param elem: Element (``ET.Element``)\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    # Filter out lines that are purely whitespace\n",
    "    return '\\n'.join([line for line in reparsed.toprettyxml(indent=\"   \").split('\\n') if line.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89798cd6",
   "metadata": {},
   "source": [
    "Configuração inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466a885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langsmith import Client\n",
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found\"\n",
    "assert os.getenv(\"ANTHROPIC_API_KEY\"), \"ANTHROPIC_API_KEY not found\"\n",
    "assert os.getenv(\"LANGSMITH_API_KEY\"), \"ANTHROPIC_API_KEY not found\"\n",
    "\n",
    "BOOK_NAME = 'LinuxFundamentals'\n",
    "EBOOKS_PATH = Path('ebooks')\n",
    "base_output_dir = EBOOKS_PATH / BOOK_NAME\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "PDF_PATH = Path('../data/701-LinuxFundamentals_material_full_v14.pdf')\n",
    "\n",
    "client = wrap_openai(OpenAI())\n",
    "# client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d459c2",
   "metadata": {},
   "source": [
    "Primeiramente é criada a classe de conversão do PDF para markdown, utiliza-se da biblioteca Docling para realizar a conversão. Essa biblioteca permite extrair as imagens e tabelas do texto, posteriormente elas são incluídas no markdown final além de serem salvas juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e84d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "import logging\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat, OutputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions\n",
    "    )\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption, MarkdownFormatOption\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem, DoclingDocument\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "class PDFConversor():\n",
    "    \"\"\"\n",
    "    Convert PDF to markdown.\n",
    "    \n",
    "    :param pdf_path: Path of the input pdf.\n",
    "    :param output_dir: Path of the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_path: Path, output_dir: Path):\n",
    "        self.input_doc_path: Path = pdf_path\n",
    "        self.base_output_dir: Path = output_dir\n",
    "        self.pipeline_options: PdfPipelineOptions = self._set_pipeline_options()\n",
    "        self.document_converter: DocumentConverter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=self.pipeline_options),\n",
    "                OutputFormat.MARKDOWN: MarkdownFormatOption(image_mode=ImageRefMode.REFERENCED)\n",
    "            },\n",
    "        )\n",
    "        self.last_page: int = self._get_no_pages()\n",
    "        self.doc = None\n",
    "       \n",
    "    def _set_pipeline_options(self) -> PdfPipelineOptions:\n",
    "        IMAGE_SCALE = 2.0\n",
    "        \n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.generate_picture_images = True\n",
    "        pipeline_options.generate_page_images = True\n",
    "        pipeline_options.images_scale = IMAGE_SCALE\n",
    "        pipeline_options.do_ocr = False\n",
    "        pipeline_options.do_table_structure = True\n",
    "        pipeline_options.table_structure_options.do_cell_matching = True\n",
    "        pipeline_options.ocr_options.lang = [\"pt\"]\n",
    "        pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "            num_threads=4, device=AcceleratorDevice.CUDA\n",
    "        )\n",
    "        return pipeline_options\n",
    "\n",
    "    \n",
    "    def _get_no_pages(self) -> int:\n",
    "        reader = PdfReader(self.input_doc_path)\n",
    "        return len(reader.pages)\n",
    "    \n",
    "    def _replace_image_placeholders(selg, md_str: str, image_files: List[Path]) -> None:\n",
    "        content = md_str\n",
    "        for img in image_files:\n",
    "            content = content.replace(\"<!-- image -->\", f\"![]({str(img).split('/')[-1]})\", 1)\n",
    "        return content\n",
    "        \n",
    "    def save_images(self, doc: DoclingDocument, output_dir: Path) -> List[str]:\n",
    "        filenames = []\n",
    "        for page in doc.pictures:\n",
    "            # print(page)\n",
    "            page_no = page.self_ref.split('/')[-1]\n",
    "            page_image_filename = output_dir / f\"{page_no}.png\"\n",
    "            print(page_image_filename)\n",
    "            with page_image_filename.open(\"wb\") as fp:\n",
    "                page.image.pil_image.save(fp, format=\"PNG\")\n",
    "            filenames.append(page_image_filename.relative_to(self.base_output_dir))\n",
    "        return filenames \n",
    "    \n",
    "    def generate_markdown(self, concepts: List[Concept]) -> None:\n",
    "        \"\"\"\n",
    "        Generate a folder for each concept, with the images captured and a ``document.md`` file.\n",
    "        \n",
    "        :param concepts: ``List[Concept]`` List of concepts, their pages must in crescent order and sequentially\n",
    "        (e.g. Chapter 1, 2, 3...).\n",
    "        \"\"\"\n",
    "        for idx in tqdm_notebook(range(len(concepts))):\n",
    "            curr_chap: Concept = concepts[idx]\n",
    "            init_page = curr_chap.page_start\n",
    "            chap_name = normalize_filename(curr_chap.concept_name)\n",
    "\n",
    "            output_concept_dir = self.base_output_dir / chap_name\n",
    "            os.makedirs(output_concept_dir, exist_ok=True)\n",
    "            \n",
    "            next_page = self.last_page + 1 if idx == len(concepts) - 1 else concepts[idx + 1].page_start - 1\n",
    "\n",
    "            doc = self.document_converter.convert(self.input_doc_path, page_range=[init_page, next_page]).document\n",
    "            md_str = doc.export_to_markdown()\n",
    "\n",
    "            img_filenames = self.save_images(doc, output_concept_dir)\n",
    "            raw_markdown = self._replace_image_placeholders(md_str, img_filenames)\n",
    "\n",
    "            with open(output_concept_dir / \"document.md\", \"w\") as f:\n",
    "                f.write(raw_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f04b8",
   "metadata": {},
   "source": [
    "``EBookExtractor`` é a classe principal, encapsulando a classe criada anteriormente e servindo como uma interface de mais alto nível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163283de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "class EbookExtractor():\n",
    "    \"\"\"\n",
    "    Extract data from an ebook.\n",
    "    \n",
    "    :param pdf_file_path: The path of the desired pdf book.\n",
    "    :param base_output_dir: Directory where the book is going to be saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_file_path: Path, base_output_dir: Path, llm: BaseChatModel):\n",
    "        self.pages: List[Document] = None\n",
    "        self.llm = llm\n",
    "        self.pdf_conversor: PDFConversor = PDFConversor(pdf_file_path, base_output_dir)\n",
    "        self.file_path: Path = pdf_file_path\n",
    "        self._load_pdf_pages()\n",
    "    \n",
    "    def _load_pdf_pages(self) -> None:\n",
    "        \"\"\"Loads PDF and returns a list of Document objects (one per page).\"\"\"\n",
    "        print(f\"Loading PDF: {self.file_path}...\")\n",
    "        loader = PyMuPDFLoader(self.file_path)\n",
    "        pages = loader.load()\n",
    "        last_page = len(pages)\n",
    "        print(f\"Loaded {len(pages)} pages.\")\n",
    "        self.pages = pages\n",
    "        \n",
    "    \n",
    "    def extract_toc_structure(self, end_toc_page = 5) -> PageExtraction:\n",
    "        \"\"\"\n",
    "        Scans the first ``end_toc_page`` pages to find a Table of Contents or Summary.\n",
    "        Returns a list of 'known concepts' to prime the main extractor.\n",
    "\n",
    "        :param end_toc_page: The first pages where the summary appears. Default to 5.\n",
    "        \"\"\"\n",
    "        print(f\"Scouting Table of Contents (Pages 1-{end_toc_page})...\")\n",
    "        \n",
    "        # Combine first pages (or fewer if small doc)\n",
    "        limit = min(len(self.pages), end_toc_page)\n",
    "        toc_text = \"\\n\".join([p.page_content for p in self.pages[:limit]])\n",
    "        \n",
    "        # Simple chain for ToC extraction\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an expert content analyzer. Look at the beginning of this book.\"),\n",
    "            (\"human\", \"\"\"Identify the Table of Contents. \n",
    "            Extract ALL chapters, sections, and sub-sections (e.g., 1.1, 1.2.1, 1.2.2) as individual Concepts.\n",
    "            Do NOT summarize or skip detailed sub-topics. Capture the full hierarchy. Do NOT include chapter number on\n",
    "            concept_name.\n",
    "            \n",
    "            Text:\n",
    "            {text}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        # We reuse the PageExtraction model, though we only care about names/start pages here\n",
    "        chain = prompt | self.llm.with_structured_output(PageExtraction)\n",
    "    \n",
    "        try:\n",
    "            result = chain.invoke({\"text\": toc_text})\n",
    "            print(f\"ToC Analysis found {len(result.concepts)} potential concepts.\")\n",
    "            return result.concepts\n",
    "        except Exception as e:\n",
    "            print(f\"Could not extract ToC (might be missing or unstructured). Proceeding with empty seed. Error: {e}\")\n",
    "            return []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9582302",
   "metadata": {},
   "source": [
    "Executando o pipeline de extração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55716ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: ../data/701-LinuxFundamentals_material_full_v14.pdf...\n",
      "Loaded 127 pages.\n",
      "Scouting Table of Contents (Pages 1-5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 11:38:02,961 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToC Analysis found 104 potential concepts.\n"
     ]
    }
   ],
   "source": [
    "llm = get_llm(Models.GPT5_1)\n",
    "extractor = EbookExtractor(PDF_PATH, base_output_dir, llm)\n",
    "toc_concepts = extractor.extract_toc_structure(end_toc_page=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed25cbe",
   "metadata": {},
   "source": [
    "Filtrando apenas os capítulos, assim podemos gerar uma pasta para cada, contendo arquivo markdown e imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30e56989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Introdução ao Linux 6\n",
      "[2] Certificações Linux 13\n",
      "[3] História do Linux 16\n",
      "[4] Licenças Open Source 20\n",
      "[5] Evolução do Linux: distribuições 23\n",
      "[6] Conhecendo o Linux 34\n",
      "[7] Tópicos para revisão do capítulo 41\n",
      "[8] Estrutura do sistema operacional 43\n",
      "[9] O que é um Shell 52\n",
      "[10] Variáveis 55\n",
      "[11] Arquivos de configuração do shell 62\n",
      "[12] Caminhos de Diretorios 68\n",
      "[13] Tópicos para revisão do capítulo 74\n",
      "[14] Como obter ajuda 76\n",
      "[15] Formas de documentação 77\n",
      "[16] Comando help 79\n",
      "[17] Comando apropos 81\n",
      "[18] Comando whatis 84\n",
      "[19] Comando man 86\n",
      "[20] Comando info 89\n",
      "[21] Comando whereis 91\n",
      "[22] Comando which 94\n",
      "[23] FHS, Hierarquia dos Diretórios 96\n",
      "[24] Aprendendo Comandos do GNU/Linux 110\n",
      "[25] Localização no sistema 120\n",
      "[26] Tópicos para revisão do capítulo 127\n"
     ]
    }
   ],
   "source": [
    "chapters = []\n",
    "for c in toc_concepts:\n",
    "    if len(c.chapter) == 1:\n",
    "        chapters.append(c)\n",
    "        print(c.chapter, c.concept_name, c.page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0fb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor.pdf_conversor.generate_markdown(chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d20085",
   "metadata": {},
   "source": [
    "# Agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54700431",
   "metadata": {},
   "source": [
    "**AgentState** será o estado utilizado pelos agentes como uma espécie de memória a curto prazo. Essa estrutura será atualizada conforme os agentes executam e capturam informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concept(BaseModel):\n",
    "    name: str = Field(description=\"The formal name of the concept.\")\n",
    "    description: str = Field(description=\"Brief definition.\")\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Concept) and self.name.lower() == other.name.lower()\n",
    "    def __hash__(self):\n",
    "        return hash(self.name.lower())\n",
    "\n",
    "class ConceptCritique(BaseModel):\n",
    "    \"\"\"Critique for a specific concept.\"\"\"\n",
    "    concept_name: str\n",
    "    is_valid: bool = Field(description=\"Is this a valid educational concept (not a stopword, generic term, or proper noun unrelated to the topic)?\")\n",
    "    reasoning: str = Field(description=\"Why valid or invalid? Mention redundancy if it exists.\")\n",
    "\n",
    "class ConceptModerationDecision(BaseModel):\n",
    "    concept_name: str\n",
    "    action: Literal['keep', 'drop', 'rename']\n",
    "    new_name: Optional[str] = Field(description=\"If action is rename, provide the new name here.\")\n",
    "    reason: str\n",
    "\n",
    "class ConceptList(BaseModel):\n",
    "    concepts: List[Concept]\n",
    "\n",
    "class ConceptCritiqueList(BaseModel):\n",
    "    critiques: List[ConceptCritique]\n",
    "\n",
    "class ConceptModerationList(BaseModel):\n",
    "    decisions: List[ConceptModerationDecision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bda5a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relation(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    relation_type: Literal['prerequisite']\n",
    "    context: Optional[str]\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (isinstance(other, Relation) and \n",
    "                self.source.lower() == other.source.lower() and \n",
    "                self.target.lower() == other.target.lower())\n",
    "\n",
    "class RelationCritique(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    is_valid: bool\n",
    "    reasoning: str\n",
    "\n",
    "class RelationModerationDecision(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    status: Literal['approved', 'rejected']\n",
    "    comments: str\n",
    "\n",
    "class RelationList(BaseModel):\n",
    "    relations: List[Relation]\n",
    "\n",
    "class RelationCritiqueList(BaseModel):\n",
    "    critiques: List[RelationCritique]\n",
    "\n",
    "class RelationModerationList(BaseModel):\n",
    "    decisions: List[RelationModerationDecision]\n",
    "    \n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"The final, polished output from the Consensus Agent.\"\"\"\n",
    "    concepts: List[Concept]\n",
    "    relations: List[Relation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f99c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    The graph state.\n",
    "    We separate the 'working memory' of each agent to preserve history and context (Rule 2).\n",
    "    \"\"\"\n",
    "    text_segment: str\n",
    "    knowledge_base: KnowledgeGraph\n",
    "    \n",
    "    # Concepts\n",
    "    extracted_concepts: List[Concept] # raw extraction\n",
    "\n",
    "    # Delta logic\n",
    "    new_concepts: List[Concept]\n",
    "    known_concepts: List[Concept]\n",
    "    \n",
    "    concept_critiques: List[ConceptCritique]\n",
    "    moderated_new_concepts: List[Concept]\n",
    "\n",
    "    active_concepts: List[Concept]\n",
    "\n",
    "    # Relation\n",
    "    proposed_relations: List[Relation]\n",
    "    \n",
    "    # Delta logic\n",
    "    new_relations: List[Relation]\n",
    "    \n",
    "    relation_critiques: List[RelationCritique]\n",
    "    moderated_new_relations: List[Relation]\n",
    "    \n",
    "    # Graph\n",
    "    final_graph_update: KnowledgeGraph\n",
    "    workflow_trace: Annotated[List[str], operator.add] # Append-only log of visited nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41121864",
   "metadata": {},
   "source": [
    "### Fase 1 - Extração de Conceitos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449d039",
   "metadata": {},
   "source": [
    "**Agente extrator de conceitos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "803b91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_extractor(state: AgentState):\n",
    "    \"\"\"\n",
    "    [NEW] Agent 0: Concept Extractor\n",
    "    Scans the text and identifies the key entities/concepts before relations are proposed.\n",
    "    \"\"\"\n",
    "    print(\"--- Concept Extractor ---\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "         You are an expert Ontology Engineer. Identify the key educational concepts or technical terms in the text.\n",
    "         \n",
    "         ### RULES: \n",
    "         1. Ignore general words.\n",
    "         2. Focus on specific subjects that would be nodes in a knowledge graph.\n",
    "         3. Concepts must be small words (such as: Linux, Integrals, Differential Equations, Kernels), avoid sentences.\n",
    "         4. Ignore generic words (e.g., 'Chapter', 'Diagram'). Focus on technical terms.\n",
    "         \"\"\"),\n",
    "        (\"user\", \"Text: {text}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm.with_structured_output(ConceptList)\n",
    "    result = chain.invoke({\"text\": state['text_segment']})\n",
    "    \n",
    "    return {\"extracted_concepts\": result.concepts, \"workflow_trace\": [\"concept_extractor\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2ba2a",
   "metadata": {},
   "source": [
    "**Agente crítico de conceitos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "571b6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_delta_filter(state: AgentState):\n",
    "    \"\"\"\n",
    "    [LOGIC NODE] Separates concepts into 'New' (need validation) and 'Known' (skip validation).\n",
    "    \"\"\"\n",
    "    print(\"--- [1.5] Concept Delta Filter ---\")\n",
    "    kb_concepts = state['knowledge_base'].concepts\n",
    "    extracted = state['extracted_concepts']\n",
    "    \n",
    "    # Simple name matching (case-insensitive via the model's __eq__)\n",
    "    known = []\n",
    "    new = []\n",
    "    \n",
    "    kb_names = {c.name.lower() for c in kb_concepts}\n",
    "    \n",
    "    for c in extracted:\n",
    "        if c.name.lower() in kb_names:\n",
    "            known.append(c)\n",
    "        else:\n",
    "            new.append(c)\n",
    "            \n",
    "    print(f\"  > Known (Skipping validation): {[c.name for c in known]}\")\n",
    "    print(f\"  > New (Sending to Critic): {[c.name for c in new]}\")\n",
    "    \n",
    "    return {\n",
    "        \"known_concepts\": known,\n",
    "        \"new_concepts\": new,\n",
    "        \"workflow_trace\": [\"concept_delta_filter\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "75e78e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_critic(state: AgentState):\n",
    "    \"\"\"Agent 2: Concept Critic\"\"\"\n",
    "    print(\"--- [2] Concept Critic ---\")\n",
    "    new_concepts = state['new_concepts']\n",
    "    \n",
    "    if not new_concepts: \n",
    "        return {\"concept_critiques\": [], \"workflow_trace\": [\"concept_critic\"]}\n",
    "\n",
    "    c_list = \", \".join([c.name for c in new_concepts])\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Critique these concepts. Mark as INVALID if they are: Too generic (e.g. 'Solution'), Verbs, or duplicates.\"),\n",
    "        (\"user\", \"Concepts: {c_list}\\n\\nCritique each:\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm.with_structured_output(ConceptCritiqueList)\n",
    "    result = chain.invoke({\"c_list\": c_list})\n",
    "    \n",
    "    return {\"concept_critiques\": result.critiques, \"workflow_trace\": [\"concept_critic\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79121e6a",
   "metadata": {},
   "source": [
    "**Agente moderador de conceitos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b76aad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_moderator(state: AgentState):\n",
    "    \"\"\"Agent 3: Concept Moderator\"\"\"\n",
    "    print(\"--- [3] Concept Moderator ---\")\n",
    "    new_concepts = state['new_concepts']\n",
    "    critiques = state['concept_critiques']\n",
    "\n",
    "    if not new_concepts:\n",
    "        return {\"moderated_new_concepts\": [], \"active_concepts\": state['known_concepts'], \"workflow_trace\": [\"concept_moderator\"]}\n",
    "\n",
    "    # Prepare Case File\n",
    "    cases = []\n",
    "    for c in new_concepts:\n",
    "        crit = next((x for x in critiques if x.concept_name == c.name), None)\n",
    "        crit_text = f\"Valid: {crit.is_valid}, Reason: {crit.reasoning}\" if crit else \"No critique.\"\n",
    "        cases.append(f\"CONCEPT: {c.name}\\nCRITIC: {crit_text}\")\n",
    "    \n",
    "    cases_str = \"\\n\".join(cases)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are the Concept Judge. Decide to 'keep', 'drop', or 'rename' concepts based on critiques.\"),\n",
    "        (\"user\", \"<CONCEPTS>:\\n{cases}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm.with_structured_output(ConceptModerationList)\n",
    "    result = chain.invoke({\"cases\": cases_str})\n",
    "    \n",
    "    # Filter and Rename\n",
    "    approved_new = []\n",
    "    for decision in result.decisions:\n",
    "        if decision.action == 'drop': continue\n",
    "        orig = next((c for c in new_concepts if c.name == decision.concept_name), None)\n",
    "        if orig:\n",
    "            if decision.action == 'rename' and decision.new_name: orig.name = decision.new_name\n",
    "            approved_new.append(orig)\n",
    "\n",
    "    active = state['known_concepts'] + approved_new\n",
    "        \n",
    "    return {\n",
    "        \"moderated_new_concepts\": approved_new, \n",
    "        \"active_concepts\": active,\n",
    "        \"workflow_trace\": [\"concept_moderator\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72223b",
   "metadata": {},
   "source": [
    "### Fase 2 - Extração de relações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aee24b",
   "metadata": {},
   "source": [
    "**Agente proponente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bc52c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_proposer(state: AgentState):\n",
    "    \"\"\"Agent 4: Relation Proposer\"\"\"\n",
    "    print(\"--- [4] Relation Proposer ---\")\n",
    "    text = state['text_segment']\n",
    "    active = state['active_concepts']\n",
    "    \n",
    "    if not active: return {\"proposed_relations\": [], \"workflow_trace\": [\"relation_proposer\"]}\n",
    "    \n",
    "    active_names = \", \".join([c.name for c in active])\n",
    "        \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "         You are an expert Knowledge Graph engineer. \n",
    "         Identify 'prerequisite' relations between the following <KNOWN CONCEPTS> based strictly on the provided <TEXT>.\n",
    "         Make sure these relations are essential (you need to know the prerequisite to understand the concept).\n",
    "         \n",
    "         ### RULES\n",
    "         1. If target concept (the prerequisite) is being presented within the <TEXT> for the first time.\n",
    "         It MUST COME before the source concept\n",
    "         \"\"\"),\n",
    "        (\"user\", \"<KNOWN CONCEPTS>: {c_names}\\n\\n<TEXT>: {text}\")\n",
    "    ])\n",
    "\n",
    "    # Enforce structured output\n",
    "    chain = prompt | llm.with_structured_output(RelationList)\n",
    "    result = chain.invoke({\"c_names\": active_names, \"text\": text})\n",
    "    \n",
    "    # Return update to state\n",
    "    return {\n",
    "        \"proposed_relations\": result.relations,\n",
    "        \"workflow_trace\": [\"relation_proposer\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d90f7",
   "metadata": {},
   "source": [
    "**Agente crítico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "db66f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_delta_filter(state: AgentState):\n",
    "    \"\"\"[LOGIC NODE] Filters out relations that already exist in the KB.\"\"\"\n",
    "    print(\"--- [4.5] Relation Delta Filter ---\")\n",
    "    proposed = state['proposed_relations']\n",
    "    kb_relations = state['knowledge_base'].relations\n",
    "    \n",
    "    new_rels = []\n",
    "    # Simple check: Source+Target equality\n",
    "    kb_set = {(r.source.lower(), r.target.lower()) for r in kb_relations}\n",
    "    \n",
    "    for r in proposed:\n",
    "        if (r.source.lower(), r.target.lower()) in kb_set:\n",
    "            print(f\"  [SKIP] Relation {r.source}->{r.target} already exists.\")\n",
    "        else:\n",
    "            new_rels.append(r)\n",
    "            \n",
    "    return {\"new_relations\": new_rels, \"workflow_trace\": [\"relation_delta_filter\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_critic(state: AgentState):\n",
    "    \"\"\"\n",
    "    Agent 5: Critic\n",
    "    Checks for inconsistency, redundancy, or hallucinations in the proposed relations.\n",
    "    \"\"\"\n",
    "    print(\"--- Critic Agent ---\")\n",
    "    \n",
    "    new_r = state['new_relations']\n",
    "    if not new_r: return {\"relation_critiques\": [], \"workflow_trace\": [\"relation_critic\"]}\n",
    "\n",
    "    # Format proposed relations for the prompt\n",
    "    proposed_str = \"\\n\".join([f\"- {r.source} -> {r.target} ({r.relation_type}): {r.context}\" for r in new_r])\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a harsh data auditor. specific <PROPOSED RELATIONS>. Check if they are supported by the text and logically sound. Look for redundancy (A->B and A->B) or cycles (A->B and B->A) which are bad for prerequisite trees.\"),\n",
    "        (\"user\", \"Text: {text}\\n\\n<PROPOSED RELATIONS>:\\n\\n{proposed_str}\\n\\nProvide a critique for EACH relation.\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm.with_structured_output(RelationCritiqueList)\n",
    "    result = chain.invoke({\"text\": state['text_segment'], \"proposed_str\": proposed_str})\n",
    "\n",
    "    return {\n",
    "        \"critiques\": result.critiques,\n",
    "        \"workflow_trace\": [\"relation_critic\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6df48",
   "metadata": {},
   "source": [
    "**Agente moderador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0eb8f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_moderator(state: AgentState):\n",
    "    \"\"\"Agent 6: Relation Moderator\"\"\"\n",
    "    print(\"--- [6] Relation Moderator ---\")\n",
    "    new_r = state['new_relations']\n",
    "    critiques = state['relation_critiques']\n",
    "    \n",
    "    # proposed = state['proposed_relations']\n",
    "    # critiques = state['relation_critiques']\n",
    "    # text = state['text_segment']\n",
    "    if not new_r: return {\"moderated_new_relations\": [], \"workflow_trace\": [\"relation_moderator\"]}\n",
    "    \n",
    "    cases = []\n",
    "    for r in new_r:\n",
    "        c = next((x for x in critiques if x.source == r.source and x.target == r.target), None)\n",
    "        cases.append(f\"REL: {r.source}->{r.target}, VALID: {c.is_valid if c else '?'}, REASON: {c.reasoning if c else ''}\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are the Moderator. Your job is to resolve conflicts between a Proposer and a Critic regarding knowledge graph relations.\"),\n",
    "        (\"user\", \"Original Text: {text}\\n\\nCASES TO JUDGE:\\n{cases}\\n\\nFor each case, decide if the relation should be KEPT (approved) or DROPPED (rejected). If the Critic found a valid error, reject it. If the Critic is being too pedantic but the relation is useful, approve it.\")\n",
    "    ])\n",
    "\n",
    "    # We expect a list of decisions\n",
    "    chain = prompt | llm.with_structured_output(RelationModerationList)\n",
    "    result = chain.invoke({\"text\": state['text_segment'], \"cases\": '\\n- '.join(cases)})\n",
    "    \n",
    "    approved = []\n",
    "    for d in result.decisions:\n",
    "        if d.status == 'approved':\n",
    "            orig = next((r for r in new_r if r.source == d.source and r.target == d.target), None)\n",
    "            if orig: approved.append(orig)\n",
    "    \n",
    "            \n",
    "    return {\"moderated_new_relations\": approved, \"workflow_trace\": [\"relation_moderator\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064cfb6",
   "metadata": {},
   "source": [
    "**Agente de consenso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5e75bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_agent(state: AgentState):\n",
    "    \"\"\"\n",
    "    Agent 7: Consensus (Refinement & Polishing)\n",
    "    Ensures high quality by merging duplicates and polishing descriptions/contexts.\n",
    "    \"\"\"\n",
    "    print(\"--- [7] Consensus Agent ---\")\n",
    "    \n",
    "    # We take the output from the moderators\n",
    "    m_concepts = state['moderated_new_concepts']\n",
    "    m_relations = state['moderated_new_relations']\n",
    "    text = state['text_segment']\n",
    "\n",
    "    if not m_concepts:\n",
    "        return {\"final_concepts\": [], \"final_relations\": [], \"workflow_trace\": [\"consensus\"]}\n",
    "\n",
    "    # Convert to string for the prompt\n",
    "    c_str = \"\\n\".join([f\"- {c.name}: {c.description}\" for c in m_concepts])\n",
    "    r_str = \"\\n\".join([f\"- {r.source} -> {r.target} (Context: {r.context})\" for r in m_relations])\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are the Final Quality Assurance Board. Your goal is to produce a pristine Knowledge Graph.\n",
    "        \n",
    "        1. Review the 'Approved Concepts': \n",
    "           - Merge synonyms (e.g., 'ANN' and 'Artificial Neural Network').\n",
    "           - Ensure descriptions are concise and high-quality.\n",
    "        \n",
    "        2. Review the 'Approved Relations': \n",
    "           - Ensure the context explains *why* A -> B. \n",
    "           - If the context is weak, improve it using the source text.\n",
    "           - Ensure the source/target names match the final concept names exactly.\n",
    "\n",
    "        3. Output the final, polished list of concepts and relations.\"\"\"),\n",
    "        (\"user\", \"Original Text: {text}\\n\\nDraft Concepts:\\n{c_str}\\n\\nDraft Relations:\\n{r_str}\\n\\nProduce Final Graph:\")\n",
    "    ])\n",
    "\n",
    "    # We expect a combined object containing both refined lists\n",
    "    chain = prompt | llm.with_structured_output(KnowledgeGraph)\n",
    "    result = chain.invoke({\"text\": text, \"c_str\": c_str, \"r_str\": r_str})\n",
    "\n",
    "    return {\n",
    "        \"final_graph_update\": result,\n",
    "        \"workflow_trace\": [\"consensus\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ea9ea",
   "metadata": {},
   "source": [
    "**Agente auditor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9c5216ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auditor_node(state: AgentState):\n",
    "    \"\"\"Agent 8: Auditor\"\"\"\n",
    "    print(\"--- [8] Auditor ---\")\n",
    "    trace = state['workflow_trace']\n",
    "    if \"consensus\" not in trace:\n",
    "        raise ValueError(\"Pipeline failed to reach Consensus.\")\n",
    "    print(\"  [AUDIT PASSED] Quality Assurance complete.\")\n",
    "    return {\"workflow_trace\": [\"auditor\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b1131",
   "metadata": {},
   "source": [
    "Workflow final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"concept_extractor\", concept_extractor)\n",
    "workflow.add_node(\"concept_delta_filter\", concept_delta_filter) # Logic\n",
    "workflow.add_node(\"concept_critic\", concept_critic)\n",
    "workflow.add_node(\"concept_moderator\", concept_moderator)\n",
    "\n",
    "workflow.add_node(\"relation_proposer\", relation_proposer)\n",
    "workflow.add_node(\"relation_delta_filter\", relation_delta_filter) # Logic\n",
    "workflow.add_node(\"relation_critic\", relation_critic)\n",
    "workflow.add_node(\"relation_moderator\", relation_moderator)\n",
    "workflow.add_node(\"consensus\", consensus_agent)\n",
    "workflow.add_node(\"auditor\", auditor_node)\n",
    "\n",
    "# Flow\n",
    "workflow.add_edge(START, \"concept_extractor\")\n",
    "workflow.add_edge(\"concept_extractor\", \"concept_delta_filter\")\n",
    "workflow.add_edge(\"concept_delta_filter\", \"concept_critic\")\n",
    "workflow.add_edge(\"concept_critic\", \"concept_moderator\")\n",
    "workflow.add_edge(\"concept_moderator\", \"relation_proposer\")\n",
    "workflow.add_edge(\"relation_proposer\", \"relation_delta_filter\")\n",
    "workflow.add_edge(\"relation_delta_filter\", \"relation_critic\")\n",
    "workflow.add_edge(\"relation_critic\", \"relation_moderator\")\n",
    "workflow.add_edge(\"relation_moderator\", \"consensus\")\n",
    "workflow.add_edge(\"consensus\", \"auditor\")\n",
    "workflow.add_edge(\"auditor\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f04cbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(run_type=\"chain\", name=\"Chapter run\")\n",
    "def invoke_extractor(initial_state: AgentState):\n",
    "    return app.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Backpropagation' description='An algorithm for computing gradients of loss functions with respect to parameters in neural networks using reverse-mode automatic differentiation.'\n",
      "name='Chain Rule' description='A rule in calculus for computing the derivative of a composite function.'\n",
      "name='Calculus' description='A branch of mathematics focused on limits, derivatives, integrals, and infinite series.'\n",
      "name='Functions' description='Mathematical mappings that assign each element of a domain to exactly one element of a codomain.'\n",
      "name='Inductive Logic' description='A branch of logic concerned with reasoning from specific observations to general conclusions or theories.'\n",
      "\n",
      "\n",
      "source='Chain Rule' target='Backpropagation' relation_type='prerequisite' context='Understanding the Chain Rule is required to follow how backpropagation computes derivatives through composed functions in a neural network.'\n",
      "source='Functions' target='Calculus' relation_type='prerequisite' context='A basic understanding of functions is required before studying calculus, since limits and derivatives are defined in terms of functions.'\n",
      "source='Calculus' target='Chain Rule' relation_type='prerequisite' context='The Chain Rule is a specific differentiation rule that belongs to the broader framework of calculus.'\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "To understand Backpropagation, one must first grasp the Chain Rule of calculus. \n",
    "However, before diving into calculus, a basic understanding of Functions is required.\n",
    "Inductive Logic is unrelated to this specific derivation tree.\n",
    "\"\"\"\n",
    "\n",
    "known_concepts = [\"Backpropagation\", \"Chain Rule\", \"Functions\", \"Inductive Logic\"]\n",
    "\n",
    "global_kb = KnowledgeGraph(concepts=[], relations=[])\n",
    "initial_state = {\n",
    "            \"text_segment\": sample_text,\n",
    "            \"knowledge_base\": global_kb,\n",
    "            # Init empty lists for required fields\n",
    "            \"extracted_concepts\": [], \"new_concepts\": [], \"known_concepts\": [],\n",
    "            \"concept_critiques\": [], \"moderated_new_concepts\": [], \"active_concepts\": [],\n",
    "            \"proposed_relations\": [], \"new_relations\": [], \"relation_critiques\": [],\n",
    "            \"moderated_new_relations\": [], \n",
    "            \"final_graph_update\": KnowledgeGraph(concepts=[], relations=[]),\n",
    "            \"workflow_trace\": []\n",
    "}\n",
    "\n",
    "\n",
    "final_state = invoke_extractor(initial_state)\n",
    "for x in final_state['final_graph_update'].concepts:\n",
    "    print(x)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for x in final_state['final_graph_update'].relations:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5b898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Validator prompt\n",
    "prompt = SystemMessage(\"\"\"You are a Strict Quality Control Agent for an Educational Knowledge Graph.\n",
    "    \n",
    "    Your Goal: Review the 'Prerequisite Relations' extracted from a text'.\n",
    "    Filter out noise to ensure high-quality graph nodes.\n",
    "    \n",
    "    **ACCEPTANCE CRITERIA**:\n",
    "    1. **Correlation**: The concepts must be strong related, such that someone would struggle to learn the source\n",
    "    concept without understand the target  \n",
    "    \"\"\")\n",
    "\n",
    "validator_agent = create_agent(\n",
    "    name=\"ValidatorAgent\",\n",
    "    model=get_llm(Models.GPT5_1),\n",
    "    middleware=[],\n",
    "    tools=[],\n",
    "    system_prompt=prompt,\n",
    "    response_format=ValidationResult\n",
    ")\n",
    "\n",
    "def invoke_validator(relations: List[Relation]) -> ValidationResult:\n",
    "    relations_str = \"\\n\".join([f\"{rel.source}-[PREREQUISITE]->{rel.target}\" for rel in relations])\n",
    "    # Pass a dict, not a HumanMessage\n",
    "    result: ValidationResult = validator_agent.invoke(input={\n",
    "        \"messages\": [HumanMessage(content=f\"Candidate relations: {relations_str}\")]\n",
    "    })\n",
    "    return result['structured_response']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5737243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "prompt = SystemMessage(\"\"\"You are a Knowledge Graph Architect. \n",
    "    Your goal is to identify the fundamental **Prerequisite Relations** within <CONTENT>.\n",
    "    You can create between <CURRENT CONCEPTS> and <PREVIOUSLY LEARNED CONCEPTS>.\n",
    "    \n",
    "    ### RULES:\n",
    "    1. If you are creating a prerequisite relation between current concepts, the\n",
    "    target must have been taught before source.\n",
    "    \n",
    "    ### YOUR TASK:\n",
    "    1. Check the relations:\n",
    "    - **prerequisite**: Does the current concept require knowing a concept from the PREVIOUSLY LEARNED CONCEPTS list? Answer yes/no and which one.\n",
    "\"\"\")\n",
    "\n",
    "creator_agent = create_agent(\n",
    "    name=\"CreatorAgent\",\n",
    "    model=get_llm(Models.GPT5_1),\n",
    "    tools=[],\n",
    "    # middleware=[SummarizationMiddleware(\n",
    "    #         model=Models.GPT5_1, \n",
    "    #         trigger=(\"tokens\", 10),\n",
    "    #         keep=(\"messages\", 20),)],\n",
    "    system_prompt=prompt,\n",
    "    response_format=ConceptAnalysis\n",
    ")\n",
    "\n",
    "def invoke_creator(current_concepts_str: str, previous_concepts_str: str, text_content: str) -> ConceptAnalysis:\n",
    "    human_message = HumanMessage(content=f\"\"\"\n",
    "        <CURRENT CONCEPTS>:\\n{current_concepts_str}\\n\\n<PREVIOUS LEARNED CONCEPTS>:\\n{previous_concepts_str}\\n\\n<CONTENT>:\\n{text_content}\"\"\")\n",
    "    return creator_agent.invoke(input={\"messages\": [human_message]})['structured_response']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49b85f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_creator_then_validator(current_concepts_str, previous_concepts_str, text_content):\n",
    "    creator_result: ConceptAnalysis = invoke_creator(current_concepts_str, previous_concepts_str, text_content)\n",
    "    validator_result: ValidationResult = invoke_validator(creator_result.relations)\n",
    "    return validator_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82cbdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_test = [\n",
    "    Concept(concept_name=\"File Permissions\", chapter=[1], description=\"\", page_start=10),\n",
    "    Concept(concept_name=\"Linux File System\", chapter=[2], description=\"\", page_start=20),\n",
    "]\n",
    "\n",
    "previous_concepts_str = \"Binary Notation, Kernel, Operating System, Drivers, Shell\"\n",
    "current_concepts_str = \"File Permissions, Linux File System\"\n",
    "\n",
    "text_content = \"\"\"In this chapter, we explore the Linux File System and its permissions model. \n",
    "Understanding file permissions is crucial for managing access to files and directories in a multi-user environment.\n",
    "File permissions in Linux are defined using a combination of read, write, and execute flags for the owner, group, and others.\"\"\"\n",
    "\n",
    "# run_creator_then_validator(current_concepts_str, previous_concepts_str, text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7c57770",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = base_output_dir\n",
    "NODES_XML = ROOT_DIRECTORY / 'nodes.xml'\n",
    "RELATIONS_XML = ROOT_DIRECTORY / 'relations.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "17106c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(concepts: List[Concept], kg: KnowledgeGraph = None, start_chapter = 0, chapters_to_extract = 1):\n",
    "    \"\"\"\n",
    "    Build the graph containing the initial concepts, new concepts learned and the relation\n",
    "    between them.\n",
    "    \"\"\"\n",
    "    # output = widgets.Output()\n",
    "    # display(output) \n",
    "    global_kb = kg\n",
    "    if kg == None:\n",
    "        global_kb = KnowledgeGraph(concepts=[], relations=[])\n",
    "    \n",
    "    output_dir = base_output_dir\n",
    "\n",
    "    files = {}\n",
    "    folder_name = ''\n",
    "    folder_count = 0\n",
    "    for idx, concept in enumerate(concepts):\n",
    "        if len(concept.chapter) == 1:\n",
    "            folder_count += 1\n",
    "            folder_name = normalize_filename(concept.concept_name)\n",
    "            files[(folder_name, folder_count)] = []\n",
    "            \n",
    "        node_id = normalize_filename(concept.concept_name)\n",
    "        files[(folder_name, folder_count)].append(concept.concept_name)\n",
    "    \n",
    "    sorted_files = sorted(files.keys(), key=lambda x: x[1])\n",
    "    files_to_extract = sorted_files[start_chapter : start_chapter + chapters_to_extract]\n",
    "    \n",
    "    for folder, idx in tqdm_notebook(files_to_extract, desc=\"Creating relations and new nodes...\"):\n",
    "        \n",
    "        # Reading content \n",
    "        file_path = os.path.join(output_dir, folder, \"document.md\")\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        \n",
    "        # with output:\n",
    "        # output.clear_output(wait=True)\n",
    "        print(f\"Analyzing: {folder}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Calling the pipeline\n",
    "        state_input = {\n",
    "            \"text_segment\": content,\n",
    "            \"knowledge_base\": global_kb,\n",
    "            # Init empty lists for required fields\n",
    "            \"extracted_concepts\": [], \"new_concepts\": [], \"known_concepts\": [],\n",
    "            \"concept_critiques\": [], \"moderated_new_concepts\": [], \"active_concepts\": [],\n",
    "            \"proposed_relations\": [], \"new_relations\": [], \"relation_critiques\": [],\n",
    "            \"moderated_new_relations\": [], \n",
    "            \"final_graph_update\": KnowledgeGraph(concepts=[], relations=[]),\n",
    "            \"workflow_trace\": []\n",
    "        }\n",
    "        \n",
    "        final_state: AgentState = invoke_extractor(state_input, langsmith_extra={\"name\": folder})\n",
    "        \n",
    "        # EXTRACT DELTA\n",
    "        delta = final_state['final_graph_update']\n",
    "        \n",
    "        print(f\"--- Chapter {idx+1} Delta ---\")\n",
    "        print(f\"New Concepts: {len(delta.concepts)}\")\n",
    "        print(f\"New Relations: {len(delta.relations)}\")\n",
    "        \n",
    "        # UPDATE GLOBAL MEMORY (Manual Merge)\n",
    "        global_kb.concepts.extend(delta.concepts)\n",
    "        global_kb.relations.extend(delta.relations)\n",
    "        \n",
    "        print(f\"--- Global KB Status ---\")\n",
    "        print(f\"Total Concepts: {len(global_kb.concepts)}\")\n",
    "        print(f\"Total Relations: {len(global_kb.relations)}\")\n",
    "        # break\n",
    "         \n",
    "    return global_kb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "262160b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9fe291220247d4a0248d7439d5d6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating relations and new nodes...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: evolucao_do_linux_distribuicoes\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:50:26,951 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['GNU/Linux', 'Kernel Linux', 'Distribuição Linux', 'GPL', 'Red Hat Enterprise Linux', 'Ubuntu', 'Linux Mint', 'Fedora', 'openSUSE', 'Android', 'DevOps']\n",
      "  > New (Sending to Critic): ['Live distribution', 'From scratch distribution', 'Distribuição derivada', 'Rolling release', 'Servidor', 'Desktop', 'RPM', 'DEB', 'Pacman', 'YUM', 'AUR', 'Red Hat Linux', 'Fedora Project', 'CentOS', 'CentOS Stream', 'CentOS Upstream', 'Oracle Linux', 'Ksplice', 'DTrace', 'Slackware Linux', 'Debian GNU/Linux', 'Ubuntu LTS', 'Canonical', 'Cinnamon', 'GNOME', 'KDE', 'Xfce', 'Kubuntu', 'Xubuntu', 'Lubuntu', 'Arch Linux', 'Manjaro Linux', 'SUSE Linux Enterprise', 'YaST', 'Knoppix', 'Gentoo Linux', 'Portage', 'Emerge', 'Metadistribuição', 'MPlayer', 'Amarok', 'OpenOffice', 'RPMFusion', 'EPEL', 'Sistema embarcado', 'Software embarcado', 'Internet das Coisas', 'Raspbian', 'Android Things', 'Debian Tinker', 'OpenWrt', 'Tizen', 'Ubuntu Core', 'Yocto Project', 'Raspberry Pi', 'Docker', 'Container Linux', 'GNOME Image Manipulation Program', 'Mozilla Firefox', 'CLI', 'GUI', 'MintMenu', 'CI/CD', 'DistroWatch', 'Kurumin', 'Kalango', 'DreamLinux', 'BrDesktop', 'Slax', 'Qubes OS', 'Kali Linux', 'KDE Plasma Desktop', 'Sistema X', 'Perl', 'Servidor web', 'Servidor FTP', 'Servidor de e-mail', 'Servidor de notícias', 'Stable', 'Testing', 'Unstable', 'Buster', 'Bullseye', 'Sid', 'SUSE', 'Novell']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:50:53,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:51:18,868 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:51:53,887 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:53:57,073 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:55:00,662 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:57:21,965 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 6 Delta ---\n",
      "New Concepts: 90\n",
      "New Relations: 97\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 200\n",
      "Total Relations: 211\n",
      "Analyzing: conhecendo_o_linux\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:58:45,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['GNU/Linux', 'Unix', 'Unix-like', 'sistema operacional', 'kernel', 'multitarefa', 'multiusuário', 'software livre', 'open source', 'GIMP', 'KDE', 'Amarok', 'Docker', 'GNOME', 'xfce']\n",
      "  > New (Sending to Critic): ['MAC OS X', 'arquivo', 'diretório', 'arquivo binário', 'arquivo de texto', 'arquivo especial', 'dispositivo de armazenamento', 'memória física', 'porta serial', 'porta paralela', 'case-sensitive', 'permissão de execução', 'shell script', 'arquivo oculto', 'ls', 'processo', 'superusuário', 'root', 'UID', 'Firefox', 'Chrome', 'Vivaldi', 'Opera', 'Thunderbird', 'SMTP', 'POP3', 'IMAP', 'S/MIME', 'criptografia de mensagens', 'certificado digital', 'desenho vetorial', 'Inkscape', 'SVG', 'DTD', 'XML', 'PNG', 'TIFF', 'GIF', 'JPG', 'AI', 'PDF', 'PS', 'Kdenlive', 'MLT', 'GNU General Public License', 'VirtualBox', 'VMware Workstation', 'virtualização', 'sistema operacional hospedeiro', 'sistema operacional convidado', 'LibreOffice', 'OpenDocument Format', 'ISO/IEC 26300', 'Microsoft Office', 'Writer', 'Calc', 'Impress', 'Draw', 'Math', 'Base', 'VLC', 'streaming', 'protocolos de streaming', 'DVD-Video', 'CD de vídeo', 'transcodificação', 'podcast', 'Audacity', 'ImageMagick', 'K3B', 'cdrecord', 'cdrdao', 'growisofs', 'CD-ROM', 'DVD', 'Blender', 'Blender Foundation', 'media player', 'smplayer', 'Audacious', 'Banshee', 'Apache HTTP Server', 'Apache Software Foundation', 'HTTPD', 'HTTP 1.1', 'API', 'NGINX', 'proxy reverso', 'proxy de e-mail', 'licença BSD', 'Lighttpd', 'c10k', 'FastCGI', 'SCGI', 'Apache Tomcat', 'Java Servlet', 'JavaServer Pages', 'Node.js', 'JavaScript', 'frontend', 'escalabilidade', 'MariaDB', 'MySQL', 'PostgreSQL', 'SQL', 'MTA', 'Postfix', 'Sendmail', 'IBM Thomas J. Watson Research Center', 'BIND', 'DNS', 'Squid', 'cache web', 'Hadoop', 'MapReduce', 'Google File System', 'Java', 'Kubernetes', 'Cloud Native Computing Foundation', 'contenedorização', 'Python', 'Python Software Foundation', 'PHP', 'Java Virtual Machine', 'bytecode', 'gerenciador de janelas', 'X Window System', 'AfterStep', 'Enlightenment', 'KWin', 'Blackbox', 'IceWM', 'Fluxbox', 'Sawfish', 'FVWM', 'Ion', 'Metacity', 'twm', 'acessibilidade']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:59:44,206 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:00:34,895 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:01:13,449 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "  [SKIP] Relation sistema operacional->kernel already exists.\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:03:19,876 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:04:22,017 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:05:54,672 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 7 Delta ---\n",
      "New Concepts: 148\n",
      "New Relations: 113\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 348\n",
      "Total Relations: 324\n",
      "Analyzing: topicos_para_revisao_do_capitulo\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:05:59,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['Linux']\n",
      "  > New (Sending to Critic): ['FHS', 'touch', 'file', 'mkdir', 'rm', 'find', 'xargs', 'locate', 'updatedb']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:06,565 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:13,429 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:15,618 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:22,848 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:26,944 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:32,680 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 8 Delta ---\n",
      "New Concepts: 4\n",
      "New Relations: 3\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 352\n",
      "Total Relations: 327\n",
      "Analyzing: estrutura_do_sistema_operacional\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:06:52,441 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['GNU/Linux', 'Sistema operacional', 'Kernel Linux', 'Bash', 'Multiusuário', 'Multitarefa', 'Unix', 'Debian GNU/Linux', 'Gerenciador de janelas', 'KDE', 'GNOME', 'Case-sensitive', 'Root']\n",
      "  > New (Sending to Critic): ['Camadas', 'Desktop environment', 'Display manager', 'ttyn', 'Hardware', 'Sessão', 'Login', 'Terminal virtual', 'Console', 'rlogin', 'ssh', 'rsh', 'rdesktop', 'telnet', 'Pseudoterminal', 'Konsole', 'Shell', 'Prompt', 'sudo', 'su', 'whoami', 'who am i', 'Variáveis de ambiente', 'Logout', 'shutdown', 'halt', 'poweroff', 'init 0', 'Sistema de arquivos', 'Nobreak', 'reboot', 'CTRL+ALT+DEL']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:07:11,282 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:07:42,208 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:08:09,445 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:09:05,458 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:09:38,004 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:10:29,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 9 Delta ---\n",
      "New Concepts: 44\n",
      "New Relations: 50\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 396\n",
      "Total Relations: 377\n",
      "Analyzing: o_que_e_um_shell\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:10:35,267 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['Shell', 'GNU/Linux', 'bash', 'UNIX', 'Shell script']\n",
      "  > New (Sending to Critic): ['Terminal de comandos', 'Bourne shell', 'sh', 'csh', 'tcsh', 'ksh', 'zsh', 'Shell de login', 'chsh']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:10:40,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:10:47,152 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:10:56,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:11:20,942 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:11:30,262 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:11:44,706 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 10 Delta ---\n",
      "New Concepts: 14\n",
      "New Relations: 15\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 410\n",
      "Total Relations: 392\n",
      "Analyzing: variaveis\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:12:00,056 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['Shell', 'Variáveis de ambiente', 'SHELL', 'bash']\n",
      "  > New (Sending to Critic): ['Variáveis', 'Variáveis locais', 'HOME', 'HOSTTYPE', 'TERM', 'USER', 'PATH', 'PS1', 'PS2', 'MAIL', 'LOGNAME', 'OSTYPE', 'echo', 'export', 'set', 'env', 'printenv', 'unset', '/etc/profile', '/etc/environment', '~/.bashrc', '~/.bash_profile', '~/.bash_login', '~/.profile']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:12:10,611 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:12:23,825 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:12:41,028 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "  [SKIP] Relation bash->Shell already exists.\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:13:36,229 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:14:03,570 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:14:41,871 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 11 Delta ---\n",
      "New Concepts: 26\n",
      "New Relations: 26\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 436\n",
      "Total Relations: 418\n",
      "Analyzing: arquivos_de_configuracao_do_shell\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:14:53,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['bash', '/etc/profile', '~/.bash_profile', '~/.bash_login', '~/.profile', '~/.bashrc', '/etc/environment', 'ls']\n",
      "  > New (Sending to Critic): ['~/.bash_logout', '/etc/bash.bashrc', 'aliases', 'alias', 'unalias', '--color', '-l', '-a', '/etc/issue', '/etc/motd', '/etc/issue.net', '.bash_history', 'HISTFILE', 'HISTSIZE', 'HISTFILESIZE', 'history', 'fc']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:15:09,112 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:15:21,607 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:15:47,003 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:16:18,340 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:16:42,302 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:17:07,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 12 Delta ---\n",
      "New Concepts: 23\n",
      "New Relations: 23\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 459\n",
      "Total Relations: 441\n",
      "Analyzing: caminhos_de_diretorios\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:17:28,490 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['Linux', 'ls', 'PATH', 'shell', 'GNU/Linux']\n",
      "  > New (Sending to Critic): ['linha de comando', 'estrutura de arquivos', 'diretório raiz', 'caminho absoluto', 'caminho relativo', 'diretório corrente', 'diretório pai', 'pwd', 'cd', 'variável de ambiente', '/bin', '/home', '/usr', '/tmp', '/etc', '/dev', '/proc', 'cpuinfo', 'home do usuário', 'curinga', 'asterisco (*)', 'ponto de interrogação (?)', 'expansão de colchetes', 'atalhos de teclado', 'clear', 'exit']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:17:40,777 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:18:00,848 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:18:18,053 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "  [SKIP] Relation Linux->GNU/Linux already exists.\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:19:00,039 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:19:38,936 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:06,811 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 13 Delta ---\n",
      "New Concepts: 30\n",
      "New Relations: 34\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 489\n",
      "Total Relations: 475\n",
      "Analyzing: topicos_para_revisao_do_capitulo\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:13,307 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['Linux', 'xargs', 'updatedb']\n",
      "  > New (Sending to Critic): ['FHS', 'Filesystem Hierarchy Standard', 'touch', 'file', 'mkdir', 'rm', 'find', 'locate']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:22,480 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:28,005 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:32,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "  [SKIP] Relation xargs->Linux already exists.\n",
      "  [SKIP] Relation updatedb->Linux already exists.\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:47,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:20:56,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:09,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 14 Delta ---\n",
      "New Concepts: 8\n",
      "New Relations: 7\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 497\n",
      "Total Relations: 482\n",
      "Analyzing: como_obter_ajuda\n",
      "--- Concept Extractor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:15,114 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1.5] Concept Delta Filter ---\n",
      "  > Known (Skipping validation): ['Sistema Operacional', 'GNU/Linux', 'Software Livre']\n",
      "  > New (Sending to Critic): ['documentação', 'serviços', 'comandos', 'Man Pages']\n",
      "--- [2] Concept Critic ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:20,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [3] Concept Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:28,527 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4] Relation Proposer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:36,105 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [4.5] Relation Delta Filter ---\n",
      "  [SKIP] Relation GNU/Linux->Sistema Operacional already exists.\n",
      "--- Critic Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:47,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [6] Relation Moderator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:51,630 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [7] Consensus Agent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:21:58,224 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [8] Auditor ---\n",
      "  [AUDIT PASSED] Quality Assurance complete.\n",
      "--- Chapter 15 Delta ---\n",
      "New Concepts: 5\n",
      "New Relations: 4\n",
      "--- Global KB Status ---\n",
      "Total Concepts: 502\n",
      "Total Relations: 486\n"
     ]
    }
   ],
   "source": [
    "res = build(toc_concepts, res ,4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5b5f1a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Concept(name='Linux', description='Sistema operacional de código aberto baseado no kernel Linux, amplamente utilizado em servidores, desktops, dispositivos móveis e na nuvem.'),\n",
       " Concept(name='Open Source', description='Modelo de desenvolvimento e licenciamento de software em que o código-fonte é aberto para uso, modificação e distribuição.'),\n",
       " Concept(name='Android', description='Sistema operacional móvel baseado no kernel Linux, amplamente utilizado em smartphones e outros dispositivos móveis.'),\n",
       " Concept(name='Sistema operacional', description='Software básico que gerencia o hardware do computador e fornece serviços para programas aplicativos.'),\n",
       " Concept(name='Segurança da informação', description='Área que trata da proteção de dados e sistemas contra acessos não autorizados, uso indevido, falhas e ataques.'),\n",
       " Concept(name='Testes de invasão', description='Prática de simular ataques a sistemas de informação para identificar vulnerabilidades de segurança, também conhecida como pentest.'),\n",
       " Concept(name='Computação em nuvem', description='Modelo de fornecimento de recursos computacionais como serviço pela Internet, incluindo infraestrutura, plataformas e software.'),\n",
       " Concept(name='Infraestrutura sob demanda', description='Provisionamento dinâmico de recursos de TI, como servidores e armazenamento, conforme a necessidade do usuário, típico da computação em nuvem.'),\n",
       " Concept(name='Amazon Web Services', description='Plataforma de computação em nuvem da Amazon, com diversos serviços de infraestrutura e plataforma.'),\n",
       " Concept(name='Google Cloud Platform', description='Plataforma de computação em nuvem do Google, que oferece serviços de infraestrutura, dados e machine learning.'),\n",
       " Concept(name='Oracle Cloud Infrastructure', description='Plataforma de computação em nuvem da Oracle voltada para infraestrutura e serviços corporativos, frequentemente abreviada como OCI.'),\n",
       " Concept(name='DigitalOcean', description='Provedor de computação em nuvem focado em simplicidade e servidores virtuais (droplets).'),\n",
       " Concept(name='Microsoft Azure', description='Plataforma de computação em nuvem da Microsoft que oferece máquinas virtuais, bancos de dados e outros serviços.'),\n",
       " Concept(name='Big Data', description='Conjunto de técnicas e tecnologias para armazenamento, processamento e análise de grandes volumes de dados.'),\n",
       " Concept(name='DevOps', description='Cultura e conjunto de práticas que integram desenvolvimento de software (Dev) e operações (Ops) para entrega contínua e ágil.'),\n",
       " Concept(name='Kernel Linux', description='Núcleo do sistema operacional Linux, responsável pela interação entre hardware e software.'),\n",
       " Concept(name='GPLv2', description='Versão 2 da GNU General Public License, licença de software livre sob a qual o kernel Linux é distribuído.'),\n",
       " Concept(name='Desktop Linux', description='Uso do sistema operacional Linux em computadores pessoais e estações de trabalho de usuários finais.'),\n",
       " Concept(name='Elementary OS', description='Distribuição Linux para desktop com foco em simplicidade e design consistente.'),\n",
       " Concept(name='MX Linux', description='Distribuição Linux baseada em Debian, voltada para leveza e estabilidade em desktops.'),\n",
       " Concept(name='Linux Mint', description='Distribuição Linux baseada em Ubuntu/Debian, voltada para facilidade de uso no desktop.'),\n",
       " Concept(name='Deepin', description='Distribuição Linux focada em experiência de uso e design de interface amigável.'),\n",
       " Concept(name='Manjaro', description='Distribuição Linux baseada em Arch Linux, focada em facilidade de instalação e uso no desktop.'),\n",
       " Concept(name='Pop!_OS', description='Distribuição Linux desenvolvida pela System76, voltada para desenvolvedores e usuários de alto desempenho.'),\n",
       " Concept(name='Ubuntu', description='Distribuição Linux baseada em Debian, amplamente utilizada em desktops, servidores e nuvem.'),\n",
       " Concept(name='Fedora', description='Distribuição Linux patrocinada pela Red Hat, focada em software livre e tecnologias recentes.'),\n",
       " Concept(name='Servidores web', description='Servidores usados para hospedar e entregar páginas e aplicações web via HTTP/HTTPS.'),\n",
       " Concept(name='W3Techs', description='Serviço que fornece estatísticas sobre o uso de tecnologias web, incluindo sistemas operacionais de servidores.'),\n",
       " Concept(name='Linux Foundation', description='Organização sem fins lucrativos que promove o desenvolvimento do Linux e de tecnologias de código aberto.'),\n",
       " Concept(name='edX', description='Plataforma de educação online que oferece cursos, inclusive na área de código aberto.'),\n",
       " Concept(name='Profissional de DevOps', description='Profissional focado em automatizar, integrar e operar ciclos de desenvolvimento e entrega de software segundo práticas DevOps.'),\n",
       " Concept(name='Certificação profissional', description='Credencial formal que valida habilidades técnicas em determinada tecnologia ou domínio.'),\n",
       " Concept(name='Tecnologia de nuvem aberta', description='Tecnologias de computação em nuvem baseadas em padrões abertos e software open source.'),\n",
       " Concept(name='Analista DevOps', description='Profissional responsável por integrar desenvolvimento e operações, gerenciando automação e pipelines em ambientes de TI.'),\n",
       " Concept(name='Site Reliability Engineering', description='Disciplina de engenharia focada em confiabilidade, disponibilidade e desempenho de sistemas em produção, conhecida como SRE.'),\n",
       " Concept(name='Analista SRE', description='Profissional que aplica práticas de Site Reliability Engineering para manter serviços confiáveis e escaláveis.'),\n",
       " Concept(name='Administrador de servidores', description='Profissional responsável por instalar, configurar, manter e monitorar servidores, muitas vezes baseados em Linux.'),\n",
       " Concept(name='Desenvolvedor de software', description='Profissional que projeta, implementa e mantém aplicações e sistemas de software.'),\n",
       " Concept(name='Engenheiro de software', description='Profissional que aplica princípios de engenharia ao desenvolvimento e manutenção de software em larga escala.'),\n",
       " Concept(name='Banco de dados', description='Sistema organizado de armazenamento e recuperação de dados estruturados.'),\n",
       " Concept(name='Analista de banco de dados', description='Profissional que projeta, administra e otimiza bancos de dados.'),\n",
       " Concept(name='Engenharia de dados', description='Área focada em projetar e manter pipelines e infraestrutura de dados para análise e aplicações.'),\n",
       " Concept(name='Engenheiro de dados', description='Profissional responsável por construir e gerenciar sistemas de processamento e integração de dados.'),\n",
       " Concept(name='Ciência de dados', description='Campo interdisciplinar que usa métodos científicos, estatísticos e computacionais para extrair conhecimento de dados.'),\n",
       " Concept(name='Cientista de dados', description='Profissional que aplica técnicas estatísticas e de machine learning para analisar dados e gerar insights.'),\n",
       " Concept(name='Machine Learning', description='Subárea de inteligência artificial que desenvolve algoritmos que aprendem padrões a partir de dados.'),\n",
       " Concept(name='Engenheiro de Machine Learning', description='Profissional que implementa, integra e mantém modelos de machine learning em produção.'),\n",
       " Concept(name='Analista de segurança', description='Profissional focado em avaliar riscos, implementar controles e monitorar a segurança de sistemas de informação.'),\n",
       " Concept(name='TI', description='Abreviação de Tecnologia da Informação, área responsável por infraestrutura, sistemas e serviços tecnológicos em organizações.'),\n",
       " Concept(name='Certificações Linux', description='Conjunto de certificações profissionais voltadas a validar conhecimentos e habilidades em sistemas operacionais Linux, oferecidas por diferentes organizações e distribuidores.'),\n",
       " Concept(name='Linux Professional Institute', description='Organização sem fins lucrativos que oferece certificações profissionais em Linux, independentes de fornecedor, baseadas no padrão Linux Standard Base.'),\n",
       " Concept(name='Red Hat', description='Empresa fornecedora de soluções Linux corporativas, incluindo o Red Hat Enterprise Linux, e um programa de certificações práticas em administração e arquitetura de sistemas.'),\n",
       " Concept(name='Linux Foundation', description='Fundação que promove o ecossistema Linux e projetos open source, oferecendo certificações profissionais com exames práticos para administradores e engenheiros Linux.'),\n",
       " Concept(name='Microsoft', description='Empresa de software e serviços que desenvolve o sistema operacional Windows e estabelece integrações e parcerias com tecnologias Linux e open source.'),\n",
       " Concept(name='SQL Server', description='Sistema de gerenciamento de banco de dados relacional da Microsoft, também disponibilizado para plataformas Linux.'),\n",
       " Concept(name='OpenSUSE', description='Distribuição Linux patrocinada pela SUSE, voltada para uso em desktops e servidores, que também oferece trilhas e certificações voltadas a seus sistemas.'),\n",
       " Concept(name='EXIN', description='Organização de certificação que oferece exames em diversas áreas de TI, incluindo uma certificação em DevOps citada no contexto de formações relacionadas a Linux.'),\n",
       " Concept(name='Linux Standard Base', description='Especificação que padroniza componentes e interfaces de sistemas Linux para garantir compatibilidade e interoperabilidade entre distribuições e aplicações.'),\n",
       " Concept(name='LPI Brasil', description='Representação nacional do Linux Professional Institute no Brasil, responsável por aplicar exames de certificação LPI no país.'),\n",
       " Concept(name='Pearson VUE', description='Plataforma e rede global de centros de testes que aplica exames de certificação profissionais, incluindo exames do Linux Professional Institute.'),\n",
       " Concept(name='Red Hat Enterprise Linux', description='Distribuição Linux corporativa da Red Hat, voltada a ambientes empresariais, usada como base para o conteúdo dos exames de certificação da empresa.'),\n",
       " Concept(name='Red Hat Certified System Administrator', description='Certificação de nível básico/intermediário da Red Hat que valida habilidades essenciais de administração do Red Hat Enterprise Linux por meio de exame prático.'),\n",
       " Concept(name='Red Hat Certified Engineer', description='Certificação avançada da Red Hat que comprova competências em administração e automação de sistemas Red Hat Enterprise Linux, avaliada por exame prático.'),\n",
       " Concept(name='Red Hat Certified Architect', description='Certificação de nível mais avançado da Red Hat, focada em arquitetura e soluções empresariais baseadas em tecnologias Red Hat.')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6140bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from pathlib import Path\n",
    "\n",
    "class GraphXMLBuilder():\n",
    "    \"\"\"Receives a KnowledgeGraph instance and builds the XML files nodes.xml and relations.xml.\"\"\"\n",
    "    def __init__(self, kg: KnowledgeGraph, output_dir: Optional[Path] = None):\n",
    "        self.kg = kg\n",
    "        self.output_dir = output_dir or Path('.')\n",
    "        self.nodes_path = self.output_dir / \"nodes.xml\"\n",
    "        self.relations_path = self.output_dir / \"relations.xml\"\n",
    "\n",
    "    def save(self):\n",
    "        import xml.etree.ElementTree as ET\n",
    "\n",
    "        # Build nodes.xml\n",
    "        root_nodes = ET.Element(\"nodes\")\n",
    "        for idx, concept in enumerate(self.kg.concepts):\n",
    "            node = ET.SubElement(root_nodes, \"node\")\n",
    "            node.set(\"id\", concept.name.replace(' ', '_').lower())\n",
    "            node.set(\"name\", concept.name)\n",
    "            node.set(\"description\", getattr(concept, 'description', ''))\n",
    "            node.set(\"order\", str(idx))\n",
    "\n",
    "        # Build relations.xml\n",
    "        root_rels = ET.Element(\"relations\")\n",
    "        for rel in self.kg.relations:\n",
    "            rel_elem = ET.SubElement(root_rels, \"relation\")\n",
    "            rel_elem.set(\"type\", rel.relation_type)\n",
    "            rel_elem.set(\"source\", rel.source.replace(' ', '_').lower())\n",
    "            rel_elem.set(\"target\", rel.target.replace(' ', '_').lower())\n",
    "            if rel.context:\n",
    "                context_elem = ET.SubElement(rel_elem, \"context\")\n",
    "                context_elem.text = rel.context\n",
    "\n",
    "        # Save pretty XML\n",
    "        xml_nodes_str = prettify_xml(root_nodes)\n",
    "        with open(self.nodes_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(xml_nodes_str)\n",
    "\n",
    "        xml_rels_str = prettify_xml(root_rels)\n",
    "        with open(self.relations_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(xml_rels_str)\n",
    "\n",
    "        print(f\"Updated {self.nodes_path} and {self.relations_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3d7bb98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ebooks/LinuxFundamentals/nodes.xml and ebooks/LinuxFundamentals/relations.xml\n"
     ]
    }
   ],
   "source": [
    "kg_builder = GraphXMLBuilder(res, base_output_dir)\n",
    "kg_builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_HTML = base_output_dir / 'graph.html'\n",
    "OUTPUT_HTML = str(OUTPUT_HTML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADSmport os\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "# Define paths (Update these if your files are in a different directory)\n",
    "NODES_XML = str(base_output_dir / \"nodes.xml\")\n",
    "RELATIONS_XML = str(base_output_dir / \"relations.xml\")\n",
    "OUTPUT_HTML = str(base_output_dir / \"graph.html\")\n",
    "\n",
    "\n",
    "def get_node_color_by_level(level_str):\n",
    "    \"\"\"Returns a color hex code based on hierarchy level.\"\"\"\n",
    "    try:\n",
    "        level = int(level_str)\n",
    "    except (ValueError, TypeError):\n",
    "        level = 1\n",
    "\n",
    "    # Palette: Deep Blue -> Teal -> Green -> Light Green\n",
    "    palette = {\n",
    "        0: \"#f0a202\", # Gold (Root)\n",
    "        1: \"#22577a\", # Dark Blue (Chapters)\n",
    "        2: \"#38a3a5\", # Teal (Sections)\n",
    "        3: \"#57cc99\", # Mint (Sub-sections)\n",
    "        4: \"#80ed99\", # Light Green\n",
    "        5: \"#c7f9cc\"  # Pale Green\n",
    "    }\n",
    "    return palette.get(level, \"#c7f9cc\") # Default to lightest for deep levels\n",
    "\n",
    "def create_interactive_graph():\n",
    "    if not os.path.exists(NODES_XML) or not os.path.exists(RELATIONS_XML):\n",
    "        print(f\"❌ XML files not found: Check {NODES_XML} and {RELATIONS_XML}\")\n",
    "        return\n",
    "\n",
    "    print(\"📊 Constructing NetworkX Graph...\")\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # 1. Parse Nodes\n",
    "    # We add nodes first to establish colors/types, but we will OVERWRITE the size later.\n",
    "    tree_nodes = ET.parse(NODES_XML)\n",
    "    for node in tree_nodes.findall(\"node\"):\n",
    "        node_id = node.get(\"id\")\n",
    "        name = node.get(\"name\")\n",
    "        node_type = node.get(\"type\", \"chapter\") \n",
    "        level = node.get(\"level\", \"1\")\n",
    "        \n",
    "        # Default styling\n",
    "        title = f\"Type: {node_type}\\nLevel: {level}\"\n",
    "        \n",
    "        if node_type == \"extracted\":\n",
    "            color = \"#ffafcc\" # Pink/Pastel for Extracted Concepts\n",
    "            found_in = node.get(\"found_in_chapter\", \"unknown\")\n",
    "            title += f\"\\nFound in: {found_in}\"\n",
    "        elif node_type == \"root\":\n",
    "            color = get_node_color_by_level(0)\n",
    "        else:\n",
    "            # Structural Node (Chapter/ToC) -> Color by Level\n",
    "            color = get_node_color_by_level(level)\n",
    "\n",
    "        # Initial add without specific size (will be calculated based on edges)\n",
    "        G.add_node(node_id, label=name, title=title, color=color, shape=\"dot\")\n",
    "\n",
    "    # 2. Parse Relations\n",
    "    tree_rels = ET.parse(RELATIONS_XML)\n",
    "    for rel in tree_rels.findall(\"relation\"):\n",
    "        source = rel.get(\"source\")\n",
    "        target = rel.get(\"target\")\n",
    "        rel_type = rel.get(\"type\")\n",
    "        \n",
    "        # --- EDGE STYLING LOGIC ---\n",
    "        color = \"#888888\" \n",
    "        width = 1\n",
    "        dashes = False\n",
    "        \n",
    "        if rel_type == \"prerequisite\":\n",
    "            color = \"#ff3366\"  # Bright Red/Pink (Critical Path)\n",
    "            width = 3\n",
    "        elif rel_type == \"part-of\":\n",
    "            color = \"#4a90e2\"  # Solid Blue\n",
    "            dashes = True      # Dashed to show hierarchy\n",
    "        elif rel_type == \"including\":\n",
    "            color = \"#00b4d8\"  # Cyan\n",
    "            dashes = True\n",
    "        elif rel_type == \"definition\":\n",
    "            color = \"#9b5de5\"  # Purple (Semantic Definition)\n",
    "            width = 2\n",
    "        elif rel_type == \"property\":\n",
    "            color = \"#f15bb5\"  # Magenta (Attribute/Property)\n",
    "            \n",
    "        if G.has_node(source) and G.has_node(target):\n",
    "            G.add_edge(source, target, title=rel_type, color=color, width=width, dashes=dashes)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 3. NEW LOGIC: Update Node Sizes based on Degree (Connections)\n",
    "    # -----------------------------------------------------------------\n",
    "    print(\"⚖️  Recalculating node sizes based on connection count...\")\n",
    "    \n",
    "    for node_id in G.nodes():\n",
    "        # Get total connections (In + Out)\n",
    "        degree = G.degree[node_id]\n",
    "        \n",
    "        # Formula: Base Size + (Degree * Multiplier)\n",
    "        # Adjust 'multiplier' (e.g., 3 or 4) to make the size difference more dramatic\n",
    "        base_size = 10\n",
    "        multiplier = 4 \n",
    "        new_size = base_size + (degree * multiplier)\n",
    "        \n",
    "        # Update the size attribute in the graph\n",
    "        G.nodes[node_id]['size'] = new_size\n",
    "        \n",
    "        # Update title to show the count on hover\n",
    "        G.nodes[node_id]['title'] += f\"\\nConnections: {degree}\"\n",
    "\n",
    "    print(f\"🕸️  Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "    # 4. Generate PyVis Visualization\n",
    "    print(\"🎨 Generating Obsidian-like HTML Visualization...\")\n",
    "    \n",
    "    # ADDED cdn_resources='in_line' to ensure dependencies load correctly\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#1e1e1e\", font_color=\"#cccccc\", select_menu=False, filter_menu=False, cdn_resources='in_line')\n",
    "    \n",
    "    net.from_nx(G)\n",
    "    \n",
    "    options = \"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 0,\n",
    "        \"borderWidthSelected\": 2,\n",
    "        \"font\": {\n",
    "          \"size\": 14,\n",
    "          \"face\": \"tahoma\",\n",
    "          \"color\": \"#eeeeee\",\n",
    "          \"strokeWidth\": 2,\n",
    "          \"strokeColor\": \"#1e1e1e\"\n",
    "        },\n",
    "        \"shadow\": {\n",
    "            \"enabled\": true,\n",
    "            \"color\": \"black\",\n",
    "            \"size\": 5,\n",
    "            \"x\": 2,\n",
    "            \"y\": 2\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"smooth\": {\n",
    "          \"type\": \"continuous\",\n",
    "          \"forceDirection\": \"none\"\n",
    "        },\n",
    "        \"arrows\": {\n",
    "            \"to\": {\n",
    "                \"enabled\": true,\n",
    "                \"scaleFactor\": 0.5\n",
    "            }\n",
    "        },\n",
    "        \"color\": {\n",
    "            \"inherit\": false,\n",
    "            \"opacity\": 1.0\n",
    "        }\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"hoverConnectedEdges\": true,\n",
    "        \"selectConnectedEdges\": true,\n",
    "        \"navigationButtons\": true,\n",
    "        \"keyboard\": true,\n",
    "        \"tooltipDelay\": 200\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"stabilization\": {\n",
    "            \"enabled\": true,\n",
    "            \"iterations\": 1000\n",
    "        },\n",
    "        \"barnesHut\": {\n",
    "          \"gravitationalConstant\": -8000,\n",
    "          \"springConstant\": 0.001,\n",
    "          \"springLength\": 200\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    net.set_options(options)\n",
    "    \n",
    "    net.save_graph(OUTPUT_HTML)\n",
    "    print(f\"✅ Visualization saved to: {os.path.abspath(OUTPUT_HTML)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "921a971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Constructing NetworkX Graph...\n",
      "⚖️  Recalculating node sizes based on connection count...\n",
      "🕸️  Graph created with 439 nodes and 485 edges.\n",
      "🎨 Generating Obsidian-like HTML Visualization...\n",
      "✅ Visualization saved to: /home/pras/Documents/LAMIA/EMBRAPII/4LINUX/notebooks/ebooks/LinuxFundamentals/graph.html\n"
     ]
    }
   ],
   "source": [
    "create_interactive_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0f0b980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def get_single_run_stats(nodes_path: str, relations_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Helper function to parse a single pair of XML files and return raw stats.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"status\": \"OK\",\n",
    "        \"nodes_total\": 0,\n",
    "        \"nodes_chapter\": 0,\n",
    "        \"nodes_extracted\": 0,\n",
    "        \"relations_total\": 0,\n",
    "        \"rel_prereq\": 0,\n",
    "        \"rel_partof\": 0,\n",
    "        \"rel_def\": 0,\n",
    "        \"rel_prop\": 0,\n",
    "        \"orphans\": 0,\n",
    "        \"avg_rel\": 0.0\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(relations_path):\n",
    "        stats[\"status\"] = \"Missing Files\"\n",
    "        return stats\n",
    "\n",
    "    try:\n",
    "        # 1. Analyze Nodes\n",
    "        tree_nodes = ET.parse(nodes_path)\n",
    "        root_nodes = tree_nodes.getroot()\n",
    "        all_nodes = root_nodes.findall(\"node\")\n",
    "        \n",
    "        stats[\"nodes_total\"] = len(all_nodes)\n",
    "        node_ids = set()\n",
    "\n",
    "        for node in all_nodes:\n",
    "            n_type = node.get(\"type\", \"chapter\")\n",
    "            if n_type == \"extracted\":\n",
    "                stats[\"nodes_extracted\"] += 1\n",
    "            else:\n",
    "                stats[\"nodes_chapter\"] += 1 # Counts root and chapters together\n",
    "            \n",
    "            node_ids.add(node.get(\"id\"))\n",
    "\n",
    "        # 2. Analyze Relations\n",
    "        tree_rels = ET.parse(relations_path)\n",
    "        root_rels = tree_rels.getroot()\n",
    "        all_rels = root_rels.findall(\"relation\")\n",
    "        \n",
    "        stats[\"relations_total\"] = len(all_rels)\n",
    "        \n",
    "        connected_nodes = set()\n",
    "\n",
    "        for rel in all_rels:\n",
    "            r_type = rel.get(\"type\", \"unknown\")\n",
    "            \n",
    "            if r_type == \"prerequisite\":\n",
    "                stats[\"rel_prereq\"] += 1\n",
    "            elif r_type == \"part-of\":\n",
    "                stats[\"rel_partof\"] += 1\n",
    "            elif r_type == \"definition\":\n",
    "                stats[\"rel_def\"] += 1\n",
    "            elif r_type == \"property\":\n",
    "                stats[\"rel_prop\"] += 1\n",
    "            \n",
    "            connected_nodes.add(rel.get(\"source\"))\n",
    "            connected_nodes.add(rel.get(\"target\"))\n",
    "\n",
    "        # 3. Health Metrics\n",
    "        orphans = node_ids - connected_nodes\n",
    "        stats[\"orphans\"] = len(orphans)\n",
    "        \n",
    "        if stats[\"nodes_total\"] > 0:\n",
    "            stats[\"avg_rel\"] = round(stats[\"relations_total\"] / stats[\"nodes_total\"], 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        stats[\"status\"] = f\"Error: {str(e)[:20]}...\"\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def analyze_graph_files(graph_runs):\n",
    "    headers = [\n",
    "        \"Generator\", \"Validator\", \"Status\", \"Nodes\", \"Chapters\", \"Concepts\",\n",
    "        \"Relations\", \"Prereqs\", \"Part-Of\", \"Defs\", \"Props\", \"Orphans\", \"Avg Rel/Node\"\n",
    "    ]\n",
    "    lines = []\n",
    "    lines.append(\"### Knowledge Graph Analysis Report\")\n",
    "    header_row = \"| \" + \" | \".join(headers) + \" |\"\n",
    "    separator = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "    lines.append(header_row)\n",
    "    lines.append(separator)\n",
    "    \n",
    "\n",
    "    for idx, (run_name, n_path, r_path) in enumerate(graph_runs, 1):\n",
    "        data = get_single_run_stats(n_path, r_path)\n",
    "        if data[\"status\"] != \"OK\":\n",
    "            row = [str(idx), data[\"status\"]] + [\"-\"] * (len(headers) - 2)\n",
    "        else:\n",
    "            row = [\n",
    "                str(run_name.split('_')[0]),\n",
    "                str(run_name.split('_')[1]),\n",
    "                \"✅ OK\",\n",
    "                str(data[\"nodes_total\"]),\n",
    "                str(data[\"nodes_chapter\"]),\n",
    "                str(data[\"nodes_extracted\"]),\n",
    "                str(data[\"relations_total\"]),\n",
    "                str(data[\"rel_prereq\"]),\n",
    "                str(data[\"rel_partof\"]),\n",
    "                str(data[\"rel_def\"]),\n",
    "                str(data[\"rel_prop\"]),\n",
    "                str(data[\"orphans\"]),\n",
    "                str(data[\"avg_rel\"])\n",
    "            ]\n",
    "        lines.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "    lines.append(\"\\n*Note: 'Chapters' includes structural nodes (Root, Sections, Chapters). 'Concepts' are LLM extracted entities.*\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac2186ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('claude-opus-4-5_gpt5-1',\n",
       "  PosixPath('graph_runs/claude-opus-4-5_gpt5-1/nodes.xml'),\n",
       "  PosixPath('graph_runs/claude-opus-4-5_gpt5-1/relations.xml')),\n",
       " ('claude-opus-4-5_claude-opus-4-5',\n",
       "  PosixPath('graph_runs/claude-opus-4-5_claude-opus-4-5/nodes.xml'),\n",
       "  PosixPath('graph_runs/claude-opus-4-5_claude-opus-4-5/relations.xml')),\n",
       " ('gpt5-1_gpt5-1',\n",
       "  PosixPath('graph_runs/gpt5-1_gpt5-1/nodes.xml'),\n",
       "  PosixPath('graph_runs/gpt5-1_gpt5-1/relations.xml')),\n",
       " ('gpt5-1_claude-opus-4-5',\n",
       "  PosixPath('graph_runs/gpt5-1_claude-opus-4-5/nodes.xml'),\n",
       "  PosixPath('graph_runs/gpt5-1_claude-opus-4-5/relations.xml'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "GRAPH_RUNS_DIR = Path(\"graph_runs\")\n",
    "graph_runs = []\n",
    "\n",
    "for run_dir in GRAPH_RUNS_DIR.iterdir():\n",
    "    if run_dir.is_dir():\n",
    "        nodes_path = run_dir / \"nodes.xml\"\n",
    "        relations_path = run_dir / \"relations.xml\"\n",
    "        if nodes_path.exists() and relations_path.exists():\n",
    "            graph_runs.append((run_dir.name, nodes_path, relations_path))\n",
    "graph_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0995fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
