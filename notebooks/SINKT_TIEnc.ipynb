{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370987af",
   "metadata": {},
   "source": [
    "# Extra√ß√£o de dados\n",
    "\n",
    "O SINKT j√° considera um dataset pronto para uso. Sendo assim essa se√ß√£o busca extrair os conceitos de um ebook PDF. Primeiramente iremos transformar em Markdown, visto que √© melhor utilizar texto puro ao inv√©s de p√°ginas de PDF. Al√©m disso, essa proposta facilita a pr√≥pria extra√ß√£o para o MAIC, posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f3c9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "\n",
    "def normalize_filename(s: str) -> str:\n",
    "    \"\"\"Remove accents, replace underscores and remove non-alphanumeric characters.\"\"\"\n",
    "    s = unicodedata.normalize('NFKD', s)\n",
    "    s = ''.join(c for c in s if not unicodedata.combining(c))\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w_]', '', s)\n",
    "    return s.lower()\n",
    "\n",
    "# def sanitize_id(text: str) -> str:\n",
    "#     \"\"\"Helper to create XML-safe IDs from concept names.\"\"\"\n",
    "#     return re.sub(r'[^a-zA-Z0-9_]', '_', text.strip())\n",
    "\n",
    "def prettify_xml(elem: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Return a pretty-printed XML string for the Element.\n",
    "    Strips the annoying extra newlines minidom likes to add.\n",
    "    \n",
    "    :param elem: Element (``ET.Element``)\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    # Filter out lines that are purely whitespace\n",
    "    return '\\n'.join([line for line in reparsed.toprettyxml(indent=\"   \").split('\\n') if line.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89798cd6",
   "metadata": {},
   "source": [
    "Configura√ß√£o inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466a885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found\"\n",
    "\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "BOOK_NAME = 'LinuxFundamentals'\n",
    "EBOOKS_PATH = Path('ebooks')\n",
    "base_output_dir = EBOOKS_PATH / BOOK_NAME\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "PDF_PATH = Path('../data/701-LinuxFundamentals_material_full_v14.pdf')\n",
    "OUTPUT_CSV = Path('../concepts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996624de",
   "metadata": {},
   "source": [
    "Definindo as estruturas de dados que iremos trabalhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f186004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Concept(BaseModel):\n",
    "    \"\"\"Represents a single educational concept found in the text.\"\"\"\n",
    "    concept_name: str = Field(description=\"The formal name of the concept (e.g., 'Inductive Logic', 'Backpropagation').\")\n",
    "    chapter: List[int] = Field(description=\"The number of the current chapter, subchapter, etc (e.g., [1] for chapter 1, [1, 2] for subchapter 1.2, [1,2,5] for subsubchapter 1.2.5)\")\n",
    "    description: str = Field(description=\"A concise definition or summary of the concept based on the text.\")\n",
    "    page_start: int = Field(description=\"The page number where this concept is first introduced.\")\n",
    "    # page_end: Optional[int] = Field(default=None, description=\"The page number where the discussion of this concept seems to end (or current page if ongoing).\")\n",
    "    is_main_chapter: bool = Field(default=False, description=\"True if this is a chapter or main topic, False if it is a subchapter or subtopic.\")\n",
    "\n",
    "class PageExtraction(BaseModel):\n",
    "    \"\"\"Container for multiple concepts found on a specific page processing step.\"\"\"\n",
    "    concepts: List[Concept] = Field(description=\"List of concepts extracted from the current text window.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d459c2",
   "metadata": {},
   "source": [
    "Primeiramente √© criada a classe de convers√£o do PDF para markdown, utiliza-se da biblioteca Docling para realizar a convers√£o. Essa biblioteca permite extrair as imagens e tabelas do texto, posteriormente elas s√£o inclu√≠das no markdown final al√©m de serem salvas juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e84d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "import logging\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat, OutputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions\n",
    "    )\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption, MarkdownFormatOption\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem, DoclingDocument\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "class PDFConversor():\n",
    "    \"\"\"\n",
    "    Convert PDF to markdown.\n",
    "    \n",
    "    :param pdf_path: Path of the input pdf.\n",
    "    :param output_dir: Path of the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_path: Path, output_dir: Path):\n",
    "        self.input_doc_path: Path = pdf_path\n",
    "        self.base_output_dir: Path = output_dir\n",
    "        self.pipeline_options: PdfPipelineOptions = self._set_pipeline_options()\n",
    "        self.document_converter: DocumentConverter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=self.pipeline_options),\n",
    "                OutputFormat.MARKDOWN: MarkdownFormatOption(image_mode=ImageRefMode.REFERENCED)\n",
    "            },\n",
    "        )\n",
    "        self.last_page: int = self._get_no_pages()\n",
    "        self.doc = None\n",
    "       \n",
    "    def _set_pipeline_options(self) -> PdfPipelineOptions:\n",
    "        IMAGE_SCALE = 2.0\n",
    "        \n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.generate_picture_images = True\n",
    "        pipeline_options.generate_page_images = True\n",
    "        pipeline_options.images_scale = IMAGE_SCALE\n",
    "        pipeline_options.do_ocr = False\n",
    "        pipeline_options.do_table_structure = True\n",
    "        pipeline_options.table_structure_options.do_cell_matching = True\n",
    "        pipeline_options.ocr_options.lang = [\"pt\"]\n",
    "        pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "            num_threads=4, device=AcceleratorDevice.CUDA\n",
    "        )\n",
    "        return pipeline_options\n",
    "\n",
    "    \n",
    "    def _get_no_pages(self) -> int:\n",
    "        reader = PdfReader(self.input_doc_path)\n",
    "        return len(reader.pages)\n",
    "    \n",
    "    def _replace_image_placeholders(selg, md_str: str, image_files: List[Path]) -> None:\n",
    "        content = md_str\n",
    "        for img in image_files:\n",
    "            content = content.replace(\"<!-- image -->\", f\"![]({str(img).split('/')[-1]})\", 1)\n",
    "        return content\n",
    "        \n",
    "    def save_images(self, doc: DoclingDocument, output_dir: Path) -> List[str]:\n",
    "        filenames = []\n",
    "        for page in doc.pictures:\n",
    "            # print(page)\n",
    "            page_no = page.self_ref.split('/')[-1]\n",
    "            page_image_filename = output_dir / f\"{page_no}.png\"\n",
    "            print(page_image_filename)\n",
    "            with page_image_filename.open(\"wb\") as fp:\n",
    "                page.image.pil_image.save(fp, format=\"PNG\")\n",
    "            filenames.append(page_image_filename.relative_to(self.base_output_dir))\n",
    "        return filenames \n",
    "    \n",
    "    def generate_markdown(self, concepts: List[Concept]) -> None:\n",
    "        \"\"\"\n",
    "        Generate a folder for each concept, with the images captured and a ``document.md`` file.\n",
    "        \n",
    "        :param concepts: ``List[Concept]`` List of concepts, their pages must in crescent order and sequentially\n",
    "        (e.g. Chapter 1, 2, 3...).\n",
    "        \"\"\"\n",
    "        for idx in tqdm_notebook(range(len(concepts))):\n",
    "            curr_chap: Concept = concepts[idx]\n",
    "            init_page = curr_chap.page_start\n",
    "            chap_name = normalize_filename(curr_chap.concept_name)\n",
    "\n",
    "            output_concept_dir = self.base_output_dir / chap_name\n",
    "            os.makedirs(output_concept_dir, exist_ok=True)\n",
    "            \n",
    "            next_page = self.last_page + 1 if idx == len(concepts) - 1 else concepts[idx + 1].page_start - 1\n",
    "\n",
    "            doc = self.document_converter.convert(self.input_doc_path, page_range=[init_page, next_page]).document\n",
    "            md_str = doc.export_to_markdown()\n",
    "\n",
    "            img_filenames = self.save_images(doc, output_concept_dir)\n",
    "            raw_markdown = self._replace_image_placeholders(md_str, img_filenames)\n",
    "\n",
    "            with open(output_concept_dir / \"document.md\", \"w\") as f:\n",
    "                f.write(raw_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f04b8",
   "metadata": {},
   "source": [
    "``EBookExtractor`` √© a classe principal, encapsulando a classe criada anteriormente e servindo como uma interface de mais alto n√≠vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163283de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class EbookExtractor():\n",
    "    \"\"\"\n",
    "    Extract data from an ebook.\n",
    "    \n",
    "    :param pdf_file_path: The path of the desired pdf book.\n",
    "    :param base_output_dir: Directory where the book is going to be saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_file_path: Path, base_output_dir: Path):\n",
    "        self.pages: List[Document] = None\n",
    "        self.llm = ChatOpenAI(temperature=0, model=MODEL_NAME)\n",
    "        self.pdf_conversor: PDFConversor = PDFConversor(pdf_file_path, base_output_dir)\n",
    "        self.file_path: Path = pdf_file_path\n",
    "        self._load_pdf_pages()\n",
    "    \n",
    "    def _load_pdf_pages(self) -> None:\n",
    "        \"\"\"Loads PDF and returns a list of Document objects (one per page).\"\"\"\n",
    "        print(f\"Loading PDF: {self.file_path}...\")\n",
    "        loader = PyMuPDFLoader(self.file_path)\n",
    "        pages = loader.load()\n",
    "        last_page = len(pages)\n",
    "        print(f\"Loaded {len(pages)} pages.\")\n",
    "        self.pages = pages\n",
    "        \n",
    "    \n",
    "    def extract_toc_structure(self, end_toc_page = 5) -> PageExtraction:\n",
    "        \"\"\"\n",
    "        Scans the first ``end_toc_page`` pages to find a Table of Contents or Summary.\n",
    "        Returns a list of 'known concepts' to prime the main extractor.\n",
    "\n",
    "        :param end_toc_page: The first pages where the summary appears. Default to 5.\n",
    "        \"\"\"\n",
    "        print(f\"Scouting Table of Contents (Pages 1-{end_toc_page})...\")\n",
    "        \n",
    "        # Combine first pages (or fewer if small doc)\n",
    "        limit = min(len(self.pages), end_toc_page)\n",
    "        toc_text = \"\\n\".join([p.page_content for p in self.pages[:limit]])\n",
    "        \n",
    "        # Simple chain for ToC extraction\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an expert content analyzer. Look at the beginning of this book.\"),\n",
    "            (\"human\", \"\"\"Identify the Table of Contents. \n",
    "            Extract ALL chapters, sections, and sub-sections (e.g., 1.1, 1.2.1, 1.2.2) as individual Concepts.\n",
    "            Do NOT summarize or skip detailed sub-topics. Capture the full hierarchy.\n",
    "            \n",
    "            Text:\n",
    "            {text}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        # We reuse the PageExtraction model, though we only care about names/start pages here\n",
    "        chain = prompt | self.llm.with_structured_output(PageExtraction)\n",
    "    \n",
    "        try:\n",
    "            result = chain.invoke({\"text\": toc_text})\n",
    "            print(f\"ToC Analysis found {len(result.concepts)} potential concepts.\")\n",
    "            return result.concepts\n",
    "        except Exception as e:\n",
    "            print(f\"Could not extract ToC (might be missing or unstructured). Proceeding with empty seed. Error: {e}\")\n",
    "            return []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9582302",
   "metadata": {},
   "source": [
    "## Executando o pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55716ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: ../data/701-LinuxFundamentals_material_full_v14.pdf...\n",
      "Loaded 127 pages.\n"
     ]
    }
   ],
   "source": [
    "extractor = EbookExtractor(PDF_PATH, base_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da935bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scouting Table of Contents (Pages 1-5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 14:15:34,150 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToC Analysis found 104 potential concepts.\n"
     ]
    }
   ],
   "source": [
    "toc_concepts = extractor.extract_toc_structure(end_toc_page=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed25cbe",
   "metadata": {},
   "source": [
    "Filtrando apenas os cap√≠tulos, assim podemos gerar uma pasta para cada, contendo arquivo markdown e imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e56989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Introdu√ß√£o ao Linux 6\n",
      "[2] CertiÔ¨Åca√ß√µes Linux 13\n",
      "[3] Hist√≥ria do Linux 16\n",
      "[4] Licen√ßas Open Source 20\n",
      "[5] Evolu√ß√£o do Linux: distribui√ß√µes 23\n",
      "[6] Conhecendo o Linux 34\n",
      "[7] T√≥picos para revis√£o do cap√≠tulo 41\n",
      "[8] Estrutura do sistema operacional 43\n",
      "[9] O que √© um Shell 52\n",
      "[10] Vari√°veis 55\n",
      "[11] Arquivos de conÔ¨Ågura√ß√£o do shell 62\n",
      "[12] Caminhos de Diretorios 68\n",
      "[13] T√≥picos para revis√£o do cap√≠tulo 74\n",
      "[14] Como obter ajuda 76\n",
      "[15] Formas de documenta√ß√£o 77\n",
      "[16] Comando help 79\n",
      "[17] Comando apropos 81\n",
      "[18] Comando whatis 84\n",
      "[19] Comando man 86\n",
      "[20] Comando info 89\n",
      "[21] Comando whereis 91\n",
      "[22] Comando which 94\n",
      "[23] FHS, Hierarquia dos Diret√≥rios 96\n",
      "[24] Aprendendo Comandos do GNU/Linux 110\n",
      "[25] Localiza√ß√£o no sistema 120\n",
      "[26] T√≥picos para revis√£o do cap√≠tulo 127\n"
     ]
    }
   ],
   "source": [
    "chapters = []\n",
    "for c in toc_concepts:\n",
    "    if len(c.chapter) == 1:\n",
    "        chapters.append(c)\n",
    "        print(c.chapter, c.concept_name, c.page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0fb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor.pdf_conversor.generate_markdown(chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36f376",
   "metadata": {},
   "source": [
    "# Gerando grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c57770",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = base_output_dir\n",
    "OUTPUT_XML = ROOT_DIRECTORY / \"global_knowledge_graph.xml\"\n",
    "INITIAL_NODES = ROOT_DIRECTORY / 'initial_nodes.xml'\n",
    "NODES_XML = ROOT_DIRECTORY / 'nodes.xml'\n",
    "RELATIONS_XML = ROOT_DIRECTORY / 'relations.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9bac45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"The subject concept.\")\n",
    "    target: str = Field(description=\"The object concept.\")\n",
    "    relation_type: Literal['prerequisite', 'including', 'part-of', 'property', 'definition']\n",
    "    context: Optional[str] = Field(description=\"Justification text.\")\n",
    "\n",
    "class ConceptAnalysis(BaseModel):\n",
    "    \"\"\"LLM Output for a full chapter/concept file.\"\"\"\n",
    "    # We map 'new_concepts' to add to registry\n",
    "    new_concepts: List[str] = Field(description=\"List of MAIN concepts defined in this text.\")\n",
    "    relations: List[Relation] = Field(description=\"Semantic connections found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75b8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalRegistry:\n",
    "    def __init__(self):\n",
    "        # Stores simple strings: {\"Binary Notation\", \"Kernel\", \"File Permissions\"}\n",
    "        self.known_concepts = set()\n",
    "\n",
    "    def add_concepts(self, concepts: List[str]):\n",
    "        for c in concepts:\n",
    "            self.known_concepts.add(c)\n",
    "    \n",
    "    def get_context_string(self):\n",
    "        \"\"\"Returns a comma-separated string of known concepts for the prompt.\"\"\"\n",
    "        return \", \".join(sorted(list(self.known_concepts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6140bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "from typing import Dict, Tuple\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "class KnowledgeGraph():    \n",
    "    def __init__(self, concepts: List[Concept], output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.nodes_path = self.output_dir / \"nodes.xml\"\n",
    "        self.relations_path = self.output_dir / \"relations.xml\"\n",
    "        \n",
    "        self.root_nodes = ET.Element(\"nodes\")\n",
    "        self.root_rels = ET.Element(\"relations\")\n",
    "        \n",
    "        self.from_toc(concepts)\n",
    "\n",
    "        self.llm = ChatOpenAI(temperature=0, model=MODEL_NAME)\n",
    "        self.registry = GlobalRegistry()\n",
    "        \n",
    "        self.node_elements = self.root_nodes.findall(\"node\")\n",
    "        self.node_elements.sort(key=lambda x: int(x.get(\"order\", 0)))\n",
    "        \n",
    "        self.existing_node_names = {node.get(\"name\").lower().strip() for node in self.node_elements}\n",
    "        self.existing_node_ids = {node.get(\"id\") for node in self.node_elements}\n",
    "\n",
    "        print(f\"Loaded {len(self.node_elements)} concepts in logical order.\")\n",
    "\n",
    "   \n",
    "    def from_toc(self, concepts: List[Concept]):\n",
    "        \"\"\"\n",
    "        Populates root_nodes and root_rels based on the Table of Contents hierarchy.\n",
    "        Does NOT write to files.\n",
    "        \"\"\"\n",
    "        print(f\"Building graph from {len(concepts)} ToC concepts...\")\n",
    "        \n",
    "        hierarchy_map: Dict[Tuple[int, ...], str] = {}\n",
    "\n",
    "        # First pass: Create Nodes\n",
    "        for idx, concept in enumerate(concepts):\n",
    "            # safe_name = normalize_filename(concept.concept_name)\n",
    "            # Using safe_name as ID (ensure concepts are unique or use f\"{idx}_{safe_name}\")\n",
    "            node_id = normalize_filename(concept.concept_name) \n",
    "            \n",
    "            hierarchy_map[tuple(concept.chapter)] = node_id\n",
    "\n",
    "            node = ET.SubElement(self.root_nodes, \"node\")\n",
    "            node.set(\"id\", node_id)\n",
    "            node.set(\"name\", concept.concept_name)\n",
    "            node.set(\"folder\", node_id) # Assumes folder matches ID/safe_name\n",
    "            node.set(\"order\", str(idx))\n",
    "            node.set(\"level\", str(len(concept.chapter)))\n",
    "            node.set(\"page_start\", str(concept.page_start))\n",
    "\n",
    "        # Second pass: Create Relations (Part-Of / Including)\n",
    "        for concept in concepts:\n",
    "            current_id = hierarchy_map.get(tuple(concept.chapter))\n",
    "            \n",
    "            if len(concept.chapter) > 1:\n",
    "                parent_key = tuple(concept.chapter[:-1])\n",
    "                parent_id = hierarchy_map.get(parent_key)\n",
    "                \n",
    "                if parent_id and current_id:\n",
    "                    # Relation 1: Parent INCLUDES Child\n",
    "                    rel1 = ET.SubElement(self.root_rels, \"relation\")\n",
    "                    rel1.set(\"type\", \"including\")\n",
    "                    rel1.set(\"source\", parent_id)\n",
    "                    rel1.set(\"target\", current_id)\n",
    "                    ET.SubElement(rel1, \"context\").text = \"Structural Hierarchy (ToC)\"\n",
    "\n",
    "                    # Relation 2: Child PART-OF Parent\n",
    "                    rel2 = ET.SubElement(self.root_rels, \"relation\")\n",
    "                    rel2.set(\"type\", \"part-of\")\n",
    "                    rel2.set(\"source\", current_id)\n",
    "                    rel2.set(\"target\", parent_id)\n",
    "                    ET.SubElement(rel2, \"context\").text = \"Structural Hierarchy (ToC)\" \n",
    "                    \n",
    "    def _append_concept(self, analysis, node_name, node_id):\n",
    "        if analysis.new_concepts:\n",
    "            new_count = 0\n",
    "            for concept in analysis.new_concepts:\n",
    "                clean_name = concept.strip()\n",
    "                clean_id = normalize_filename(clean_name)\n",
    "                if clean_id not in self.existing_node_ids and clean_name.lower() not in self.existing_node_names:\n",
    "                    # Create new Node entry\n",
    "                    new_node = ET.SubElement(self.root_nodes, \"node\")\n",
    "                    new_node.set(\"id\", clean_id)\n",
    "                    new_node.set(\"name\", clean_name)\n",
    "                    new_node.set(\"type\", \"extracted\")\n",
    "                    new_node.set(\"found_in_chapter\", node_id)\n",
    "                    \n",
    "                    self.existing_node_names.add(clean_name.lower())\n",
    "                    self.existing_node_ids.add(clean_id)\n",
    "                    new_count += 1\n",
    "        \n",
    "            self.registry.add_concepts(analysis.new_concepts)\n",
    "            print(f\"Learned: {len(analysis.new_concepts)} concepts ({new_count} new to XML)\")\n",
    "            \n",
    "            # The current node itself is now 'known'\n",
    "            self.registry.add_concepts([node_name])\n",
    "            \n",
    "    def _append_relation(self, analysis, node_id):\n",
    "        for sem_rel in analysis.relations:\n",
    "            target_id = normalize_filename(sem_rel.target)\n",
    "            \n",
    "            rel_elem = ET.SubElement(self.root_rels, \"relation\")\n",
    "            rel_elem.set(\"type\", sem_rel.relation_type)\n",
    "            \n",
    "            if sem_rel.relation_type == 'prerequisite':\n",
    "                # Prerequisite Flow: The concept in history (Target of extraction) -> Current Node\n",
    "                rel_elem.set(\"source\", target_id) # The old concept\n",
    "                rel_elem.set(\"target\", node_id)        # The current concept\n",
    "            else:\n",
    "                # Definition/Property Flow: Current Node -> Attribute\n",
    "                rel_elem.set(\"source\", node_id)\n",
    "                rel_elem.set(\"target\", target_id)\n",
    "            \n",
    "            ET.SubElement(rel_elem, \"context\").text = sem_rel.context\n",
    "            \n",
    "    def analyze_concept_content(self, current_node_name, text_content):\n",
    "        \"\"\"\n",
    "        Analyzes the entire markdown content for a specific concept node.\n",
    "        \n",
    "        :param current_node_name: Name of the current node.\n",
    "        :param text_content: Text content.\n",
    "        :param registry: The current state of the registry (know concepts).\n",
    "        \"\"\"\n",
    "        if not text_content.strip():\n",
    "            return ConceptAnalysis(new_concepts=[], relations=[])\n",
    "\n",
    "        previous_concepts_str = self.registry.get_context_string()\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a Knowledge Graph Expert.\n",
    "            \n",
    "            Current Concept: '{current_node}'\n",
    "            \n",
    "            Your Goal:\n",
    "            1. List NEW concepts explicitly taught/defined here.\n",
    "            2. Extract Semantic Relations:\n",
    "            - **definition**: If '{current_node}' is defined here.\n",
    "            - **property**: Key attributes of '{current_node}'.\n",
    "            - **prerequisite**: Does this text require knowing a concept from the PREVIOUSLY LEARNED list?\n",
    "            \n",
    "            PREVIOUSLY LEARNED CONCEPTS:\n",
    "            [{history}]\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{text}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.llm.with_structured_output(ConceptAnalysis)\n",
    "        \n",
    "        try:\n",
    "            return chain.invoke({\n",
    "                \"current_node\": current_node_name, \n",
    "                \"text\": text_content[:15000], # Safety cap for tokens\n",
    "                \"history\": previous_concepts_str\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error: {e}\")\n",
    "            return ConceptAnalysis(new_concepts_taught=[], relations=[])\n",
    "        \n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build the graph containing the initial concepts, new concepts learned and the relation\n",
    "        between them.\n",
    "        \"\"\"\n",
    "        output = widgets.Output()\n",
    "        display(output) \n",
    "        \n",
    "        for node in tqdm_notebook(self.node_elements, desc=\"Creating relations and new nodes...\"):\n",
    "            node_id = node.get(\"id\")\n",
    "            node_name = node.get(\"name\")\n",
    "            folder_name = node.get(\"folder\")\n",
    "            \n",
    "            # Path to the granular MD file\n",
    "            file_path = os.path.join(ROOT_DIRECTORY, folder_name, \"document.md\")\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "            \n",
    "            with output:\n",
    "                output.clear_output(wait=True)\n",
    "                print(f\"Analyzing: {node_name}\")\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                analysis = self.analyze_concept_content(node_name, content)\n",
    "                self._append_concept(analysis, node_name, node_id) # add concepts            \n",
    "                self._append_relation(analysis, node_id) # Add relations\n",
    "            \n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Save the final XML files.\n",
    "        \"\"\"\n",
    "        xml_rels_str = prettify_xml(self.root_rels)\n",
    "        with open(RELATIONS_XML, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(xml_rels_str)\n",
    "            \n",
    "        xml_nodes_str = prettify_xml(self.root_nodes)\n",
    "        with open(NODES_XML, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(xml_nodes_str)\n",
    "            \n",
    "        print(f\"Updated {NODES_XML} and {RELATIONS_XML}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "395fdc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph from 104 ToC concepts...\n",
      "Loaded 104 concepts in logical order.\n"
     ]
    }
   ],
   "source": [
    "kgx = KnowledgeGraph(toc_concepts, base_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44d9499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b783630cf90347a486369710bf86761c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89384bea1ec4838adf87bf2d6494ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating relations and new nodes...:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kgx.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad28eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ebooks/LinuxFundamentals/nodes.xml and ebooks/LinuxFundamentals/relations.xml\n"
     ]
    }
   ],
   "source": [
    "kgx.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e7a7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_HTML = base_output_dir / 'graph.html'\n",
    "OUTPUT_HTML = str(OUTPUT_HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ce0fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def create_interactive_graph():\n",
    "    if not os.path.exists(NODES_XML) or not os.path.exists(RELATIONS_XML):\n",
    "        print(\"‚ùå XML files not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Constructing NetworkX Graph...\")\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # 1. Parse Nodes\n",
    "    tree_nodes = ET.parse(NODES_XML)\n",
    "    for node in tree_nodes.findall(\"node\"):\n",
    "        node_id = node.get(\"id\")\n",
    "        name = node.get(\"name\")\n",
    "        node_type = node.get(\"type\", \"chapter\") \n",
    "        \n",
    "        # Obsidian Style: Dots with specific colors\n",
    "        color = \"#8bd3dd\"  # Cyan/Blue for Chapters\n",
    "        size = 15          # Standard size\n",
    "        title = f\"Type: {node_type}\"\n",
    "        \n",
    "        if node_type == \"extracted\":\n",
    "            color = \"#ffafcc\" # Pink/Pastel for Concepts\n",
    "            size = 10         # Smaller for concepts\n",
    "            found_in = node.get(\"found_in_chapter\", \"unknown\")\n",
    "            title += f\"\\nFound in: {found_in}\"\n",
    "        elif node_type == \"root\":\n",
    "            color = \"#f0a202\" # Gold for Root\n",
    "            size = 25\n",
    "\n",
    "        G.add_node(node_id, label=name, title=title, color=color, size=size, shape=\"dot\")\n",
    "\n",
    "    # 2. Parse Relations\n",
    "    tree_rels = ET.parse(RELATIONS_XML)\n",
    "    for rel in tree_rels.findall(\"relation\"):\n",
    "        source = rel.get(\"source\")\n",
    "        target = rel.get(\"target\")\n",
    "        rel_type = rel.get(\"type\")\n",
    "        \n",
    "        # Edges: Visibility Fix -> Brighter, Solid Colors\n",
    "        # color = \"#666666\" # Solid lighter gray for default edges\n",
    "        # width = 1\n",
    "        # dashes = False\n",
    "        color = \"#4a90e2\" # Solid Blue (instead of faint cyan)\n",
    "        dashes = True     # Keep dashes to distinguish structure\n",
    "        width = 2\n",
    "        \n",
    "        if rel_type == \"prerequisite\":\n",
    "          color = \"#ff4d6d\" # Bright Red/Pink\n",
    "          width = 3         # Thicker to stand out\n",
    "        # elif rel_type in [\"part-of\", \"including\"]:\n",
    "\n",
    "        # if G.has_node(source) and G.has_node(target):\n",
    "        G.add_edge(source, target, title=rel_type, color=color, width=width, dashes=dashes)\n",
    "\n",
    "    print(f\"üï∏Ô∏è  Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "    # 3. Generate PyVis Visualization (Obsidian Style)\n",
    "    print(\"üé® Generating Obsidian-like HTML Visualization...\")\n",
    "    \n",
    "    # Removed filter_menu and select_menu as requested\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#1e1e1e\", font_color=\"#cccccc\", select_menu=False, filter_menu=False)\n",
    "    \n",
    "    net.from_nx(G)\n",
    "    \n",
    "    # Physics & Interaction Options\n",
    "    # Removed transparency from shadow colors to improve crispness\n",
    "    options = \"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 0,\n",
    "        \"borderWidthSelected\": 2,\n",
    "        \"font\": {\n",
    "          \"size\": 14,\n",
    "          \"face\": \"tahoma\",\n",
    "          \"color\": \"#eeeeee\",\n",
    "          \"strokeWidth\": 2,\n",
    "          \"strokeColor\": \"#1e1e1e\"\n",
    "        },\n",
    "        \"shadow\": {\n",
    "            \"enabled\": true,\n",
    "            \"color\": \"black\",\n",
    "            \"size\": 5,\n",
    "            \"x\": 2,\n",
    "            \"y\": 2\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"smooth\": {\n",
    "          \"type\": \"continuous\",\n",
    "          \"forceDirection\": \"none\"\n",
    "        },\n",
    "        \"arrows\": {\n",
    "            \"to\": {\n",
    "                \"enabled\": true,\n",
    "                \"scaleFactor\": 0.5\n",
    "            }\n",
    "        },\n",
    "        \"color\": {\n",
    "            \"inherit\": false,\n",
    "            \"opacity\": 1.0\n",
    "        }\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"hoverConnectedEdges\": true,\n",
    "        \"selectConnectedEdges\": true,\n",
    "        \"navigationButtons\": true,\n",
    "        \"keyboard\": true,\n",
    "        \"tooltipDelay\": 200\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    net.set_options(options)\n",
    "    \n",
    "    # net.show_buttons(filter_=['physics'])\n",
    "\n",
    "    # Save\n",
    "    net.save_graph(OUTPUT_HTML)\n",
    "    print(f\"‚úÖ Visualization saved to: {os.path.abspath(OUTPUT_HTML)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "921a971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Constructing NetworkX Graph...\n",
      "üï∏Ô∏è  Graph created with 299 nodes and 394 edges.\n",
      "üé® Generating Obsidian-like HTML Visualization...\n",
      "‚úÖ Visualization saved to: /home/pras/EMBRAPII/4linux/notebooks/ebooks/LinuxFundamentals/graph.html\n"
     ]
    }
   ],
   "source": [
    "create_interactive_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b980c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
