{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370987af",
   "metadata": {},
   "source": [
    "# Extra√ß√£o de dados\n",
    "\n",
    "O SINKT j√° considera um dataset pronto para uso. Sendo assim essa se√ß√£o busca extrair os conceitos de um ebook PDF. Primeiramente iremos transformar em Markdown, visto que √© melhor utilizar texto puro ao inv√©s de p√°ginas de PDF. Al√©m disso, essa proposta facilita a pr√≥pria extra√ß√£o para o MAIC, posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3c9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_filename(s):\n",
    "    \"\"\"Remove accents, replace underscores and remove non-alphanumeric characters.\"\"\"\n",
    "    s = unicodedata.normalize('NFKD', s)\n",
    "    s = ''.join(c for c in s if not unicodedata.combining(c))\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w_]', '', s)\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89798cd6",
   "metadata": {},
   "source": [
    "Configura√ß√£o inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466a885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found\"\n",
    "\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "BOOK_NAME = 'LinuxFundamentals'\n",
    "EBOOKS_PATH = Path('ebooks')\n",
    "base_output_dir = EBOOKS_PATH / BOOK_NAME\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "PDF_PATH = Path('../data/701-LinuxFundamentals_material_full_v14.pdf')\n",
    "OUTPUT_CSV = Path('../concepts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996624de",
   "metadata": {},
   "source": [
    "Definindo as estruturas de dados que iremos trabalhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f186004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Concept(BaseModel):\n",
    "    \"\"\"Represents a single educational concept found in the text.\"\"\"\n",
    "    concept_name: str = Field(description=\"The formal name of the concept (e.g., 'Inductive Logic', 'Backpropagation').\")\n",
    "    chapter: List[int] = Field(description=\"The number of the current chapter, subchapter, etc (e.g., [1] for chapter 1, [1, 2] for subchapter 1.2, [1,2,5] for subsubchapter 1.2.5)\")\n",
    "    description: str = Field(description=\"A concise definition or summary of the concept based on the text.\")\n",
    "    page_start: int = Field(description=\"The page number where this concept is first introduced.\")\n",
    "    # page_end: Optional[int] = Field(default=None, description=\"The page number where the discussion of this concept seems to end (or current page if ongoing).\")\n",
    "    is_main_chapter: bool = Field(default=False, description=\"True if this is a chapter or main topic, False if it is a subchapter or subtopic.\")\n",
    "\n",
    "class PageExtraction(BaseModel):\n",
    "    \"\"\"Container for multiple concepts found on a specific page processing step.\"\"\"\n",
    "    concepts: List[Concept] = Field(description=\"List of concepts extracted from the current text window.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda4ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, Tuple \n",
    "from xml.dom import minidom\n",
    "\n",
    "class StructuralGraphBuilder:\n",
    "    \"\"\"\n",
    "    Responsible for creating the initial 'part-of' and 'including' relationships\n",
    "    based strictly on the Table of Contents structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def build_graph(self, concepts: List[Concept]) -> Tuple[Path, Path]:\n",
    "        \"\"\"\n",
    "        Generates two XML files:\n",
    "        1. nodes.xml: Definitions of concepts/chapters.\n",
    "        2. relations.xml: Relationships referencing node IDs.\n",
    "        \"\"\"\n",
    "        nodes_root = ET.Element(\"nodes\")\n",
    "        relations_root = ET.Element(\"relations\")\n",
    "        \n",
    "        # Map tuple(chapter_list) -> node_id for easy parent lookup\n",
    "        # e.g. (1, 1) -> \"1_History\"\n",
    "        hierarchy_map: Dict[Tuple[int, ...], str] = {}\n",
    "\n",
    "        print(f\"üèóÔ∏è Building Graph for {len(concepts)} concepts...\")\n",
    "\n",
    "        # First pass: Create all nodes and populate map\n",
    "        for idx, concept in enumerate(concepts):\n",
    "            safe_name = normalize_filename(concept.concept_name)\n",
    "            node_id = f\"{safe_name}\"\n",
    "            \n",
    "            # Save to map for relationship building\n",
    "            hierarchy_map[tuple(concept.chapter)] = node_id\n",
    "\n",
    "            # Create Node Element\n",
    "            node = ET.SubElement(nodes_root, \"node\")\n",
    "            node.set(\"id\", node_id)\n",
    "            node.set(\"name\", concept.concept_name)\n",
    "            node.set(\"folder\", node_id)\n",
    "            node.set(\"order\", str(idx))\n",
    "            node.set(\"level\", str(len(concept.chapter)))\n",
    "            node.set(\"page_start\", str(concept.page_start))\n",
    "\n",
    "        # Second pass: Build relationships based on hierarchy\n",
    "        for concept in concepts:\n",
    "            current_id = hierarchy_map[tuple(concept.chapter)]\n",
    "            \n",
    "            # Infer Parent based on chapter list\n",
    "            # If current is [1, 2, 1], parent should be [1, 2]\n",
    "            if len(concept.chapter) > 1:\n",
    "                parent_key = tuple(concept.chapter[:-1])\n",
    "                parent_id = hierarchy_map.get(parent_key)\n",
    "                \n",
    "                if parent_id:\n",
    "                    # Relation 1: Parent INCLUDES Child\n",
    "                    rel1 = ET.SubElement(relations_root, \"relation\")\n",
    "                    rel1.set(\"type\", \"including\")\n",
    "                    rel1.set(\"source\", parent_id)\n",
    "                    rel1.set(\"target\", current_id)\n",
    "                    ET.SubElement(rel1, \"context\").text = \"Structural Hierarchy (ToC)\"\n",
    "\n",
    "                    # Relation 2: Child PART-OF Parent\n",
    "                    rel2 = ET.SubElement(relations_root, \"relation\")\n",
    "                    rel2.set(\"type\", \"part-of\")\n",
    "                    rel2.set(\"source\", current_id)\n",
    "                    rel2.set(\"target\", parent_id)\n",
    "                    ET.SubElement(rel2, \"context\").text = \"Structural Hierarchy (ToC)\"\n",
    "\n",
    "        # Save Nodes XML\n",
    "        nodes_str = minidom.parseString(ET.tostring(nodes_root)).toprettyxml(indent=\"   \")\n",
    "        nodes_path = self.output_dir / \"initial_nodes.xml\"\n",
    "        with open(nodes_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(nodes_str)\n",
    "\n",
    "        # Save Relations XML\n",
    "        relations_str = minidom.parseString(ET.tostring(relations_root)).toprettyxml(indent=\"   \")\n",
    "        relations_path = self.output_dir / \"relations.xml\"\n",
    "        with open(relations_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(relations_str)\n",
    "            \n",
    "        print(f\"‚úÖ Graph saved: {nodes_path} and {relations_path}\")\n",
    "        return nodes_path, relations_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d459c2",
   "metadata": {},
   "source": [
    "Primeiramente √© criada a classe de convers√£o do PDF para markdown, utiliza-se da biblioteca Docling para realizar a convers√£o. Essa biblioteca permite extrair as imagens e tabelas do texto, posteriormente elas s√£o inclu√≠das no markdown final al√©m de serem salvas juntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e84d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "import logging\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat, OutputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions\n",
    "    )\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption, MarkdownFormatOption\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem, DoclingDocument\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "class PDFConversor():\n",
    "    \"\"\"\n",
    "    Convert PDF to markdown.\n",
    "    \n",
    "    :param pdf_path: Path of the input pdf.\n",
    "    :param output_dir: Path of the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_path: Path, output_dir: Path):\n",
    "        self.input_doc_path: Path = pdf_path\n",
    "        self.base_output_dir: Path = output_dir\n",
    "        self.pipeline_options: PdfPipelineOptions = self._set_pipeline_options()\n",
    "        self.document_converter: DocumentConverter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=self.pipeline_options),\n",
    "                OutputFormat.MARKDOWN: MarkdownFormatOption(image_mode=ImageRefMode.REFERENCED)\n",
    "            },\n",
    "        )\n",
    "        self.last_page: int = self._get_no_pages()\n",
    "        self.doc = None\n",
    "       \n",
    "    def _set_pipeline_options(self) -> PdfPipelineOptions:\n",
    "        IMAGE_SCALE = 2.0\n",
    "        \n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.generate_picture_images = True\n",
    "        pipeline_options.generate_page_images = True\n",
    "        pipeline_options.images_scale = IMAGE_SCALE\n",
    "        pipeline_options.do_ocr = False\n",
    "        pipeline_options.do_table_structure = True\n",
    "        pipeline_options.table_structure_options.do_cell_matching = True\n",
    "        pipeline_options.ocr_options.lang = [\"pt\"]\n",
    "        pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "            num_threads=4, device=AcceleratorDevice.CUDA\n",
    "        )\n",
    "        return pipeline_options\n",
    "\n",
    "    \n",
    "    def _get_no_pages(self) -> int:\n",
    "        reader = PdfReader(self.input_doc_path)\n",
    "        return len(reader.pages)\n",
    "    \n",
    "    def _replace_image_placeholders(selg, md_str: str, image_files: List[Path]) -> None:\n",
    "        content = md_str\n",
    "        for img in image_files:\n",
    "            content = content.replace(\"<!-- image -->\", f\"![]({str(img).split('/')[-1]})\", 1)\n",
    "        return content\n",
    "        \n",
    "    def save_images(self, doc: DoclingDocument, output_dir: Path) -> List[str]:\n",
    "        filenames = []\n",
    "        for page in doc.pictures:\n",
    "            # print(page)\n",
    "            page_no = page.self_ref.split('/')[-1]\n",
    "            page_image_filename = output_dir / f\"{page_no}.png\"\n",
    "            print(page_image_filename)\n",
    "            with page_image_filename.open(\"wb\") as fp:\n",
    "                page.image.pil_image.save(fp, format=\"PNG\")\n",
    "            filenames.append(page_image_filename.relative_to(self.base_output_dir))\n",
    "        return filenames \n",
    "    \n",
    "    def generate_markdown(self, concepts: List[Concept]) -> None:\n",
    "        \"\"\"\n",
    "        Generate a folder for each concept, with the images captured and a ``document.md`` file.\n",
    "        \n",
    "        :param concepts: ``List[Concept]`` List of concepts, their pages must in crescent order and sequentially\n",
    "        (e.g. Chapter 1, 2, 3...).\n",
    "        \"\"\"\n",
    "        for idx in tqdm_notebook(range(len(concepts))):\n",
    "            curr_chap: Concept = concepts[idx]\n",
    "            init_page = curr_chap.page_start\n",
    "            chap_name = normalize_filename(curr_chap.concept_name)\n",
    "\n",
    "            output_concept_dir = self.base_output_dir / chap_name\n",
    "            os.makedirs(output_concept_dir, exist_ok=True)\n",
    "            \n",
    "            next_page = self.last_page + 1 if idx == len(concepts) - 1 else concepts[idx + 1].page_start - 1\n",
    "\n",
    "            doc = self.document_converter.convert(self.input_doc_path, page_range=[init_page, next_page]).document\n",
    "            md_str = doc.export_to_markdown()\n",
    "\n",
    "            img_filenames = self.save_images(doc, output_concept_dir)\n",
    "            raw_markdown = self._replace_image_placeholders(md_str, img_filenames)\n",
    "\n",
    "            with open(output_concept_dir / \"document.md\", \"w\") as f:\n",
    "                f.write(raw_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f04b8",
   "metadata": {},
   "source": [
    "``EBookExtractor`` √© a classe principal, encapsulando a classe criada anteriormente e servindo como uma interface de mais alto n√≠vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163283de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class EbookExtractor():\n",
    "    \"\"\"\n",
    "    Extract data from an ebook.\n",
    "    \n",
    "    :param pdf_file_path: The path of the desired pdf book.\n",
    "    :param base_output_dir: Directory where the book is going to be saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_file_path: Path, base_output_dir: Path):\n",
    "        self.pages: List[Document] = None\n",
    "        self.llm = ChatOpenAI(temperature=0, model=MODEL_NAME)\n",
    "        self.pdf_conversor: PDFConversor = PDFConversor(pdf_file_path, base_output_dir)\n",
    "        self.file_path: Path = pdf_file_path\n",
    "        self._load_pdf_pages()\n",
    "    \n",
    "    def _load_pdf_pages(self) -> None:\n",
    "        \"\"\"Loads PDF and returns a list of Document objects (one per page).\"\"\"\n",
    "        print(f\"Loading PDF: {self.file_path}...\")\n",
    "        loader = PyMuPDFLoader(self.file_path)\n",
    "        pages = loader.load()\n",
    "        last_page = len(pages)\n",
    "        print(f\"Loaded {len(pages)} pages.\")\n",
    "        self.pages = pages\n",
    "        \n",
    "    \n",
    "    def extract_toc_structure(self, end_toc_page = 5) -> PageExtraction:\n",
    "        \"\"\"\n",
    "        Scans the first ``end_toc_page`` pages to find a Table of Contents or Summary.\n",
    "        Returns a list of 'known concepts' to prime the main extractor.\n",
    "\n",
    "        :param end_toc_page: The first pages where the summary appears. Default to 5.\n",
    "        \"\"\"\n",
    "        print(f\"Scouting Table of Contents (Pages 1-{end_toc_page})...\")\n",
    "        \n",
    "        # Combine first pages (or fewer if small doc)\n",
    "        limit = min(len(self.pages), end_toc_page)\n",
    "        toc_text = \"\\n\".join([p.page_content for p in self.pages[:limit]])\n",
    "        \n",
    "        # Simple chain for ToC extraction\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an expert content analyzer. Look at the beginning of this book.\"),\n",
    "            (\"human\", \"\"\"Identify the Table of Contents. \n",
    "            Extract ALL chapters, sections, and sub-sections (e.g., 1.1, 1.2.1, 1.2.2) as individual Concepts.\n",
    "            Do NOT summarize or skip detailed sub-topics. Capture the full hierarchy.\n",
    "            \n",
    "            Text:\n",
    "            {text}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        # We reuse the PageExtraction model, though we only care about names/start pages here\n",
    "        chain = prompt | self.llm.with_structured_output(PageExtraction)\n",
    "    \n",
    "        try:\n",
    "            result = chain.invoke({\"text\": toc_text})\n",
    "            print(f\"üìã ToC Analysis found {len(result.concepts)} potential concepts.\")\n",
    "            return result.concepts\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not extract ToC (might be missing or unstructured). Proceeding with empty seed. Error: {e}\")\n",
    "            return []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9582302",
   "metadata": {},
   "source": [
    "## Executando o pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55716ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: ../data/701-LinuxFundamentals_material_full_v14.pdf...\n",
      "Loaded 127 pages.\n"
     ]
    }
   ],
   "source": [
    "extractor = EbookExtractor(PDF_PATH, base_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da935bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scouting Table of Contents (Pages 1-5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 22:53:59,864 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã ToC Analysis found 104 potential concepts.\n"
     ]
    }
   ],
   "source": [
    "toc_concepts = extractor.extract_toc_structure(end_toc_page=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed25cbe",
   "metadata": {},
   "source": [
    "Filtrando apenas os cap√≠tulos, assim podemos gerar uma pasta para cada, contendo arquivo markdown e imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e56989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Introdu√ß√£o ao Linux 6\n",
      "[2] CertiÔ¨Åca√ß√µes Linux 13\n",
      "[3] Hist√≥ria do Linux 16\n",
      "[4] Licen√ßas Open Source 20\n",
      "[5] Evolu√ß√£o do Linux: distribui√ß√µes 23\n",
      "[6] Conhecendo o Linux 34\n",
      "[7] T√≥picos para revis√£o do cap√≠tulo 41\n",
      "[8] Estrutura do sistema operacional 43\n",
      "[9] O que √© um Shell 52\n",
      "[10] Vari√°veis 55\n",
      "[11] Arquivos de conÔ¨Ågura√ß√£o do shell 62\n",
      "[12] Caminhos de Diretorios 68\n",
      "[13] T√≥picos para revis√£o do cap√≠tulo 74\n",
      "[14] Como obter ajuda 76\n",
      "[15] Formas de documenta√ß√£o 77\n",
      "[16] Comando help 79\n",
      "[17] Comando apropos 81\n",
      "[18] Comando whatis 84\n",
      "[19] Comando man 86\n",
      "[20] Comando info 89\n",
      "[21] Comando whereis 91\n",
      "[22] Comando which 94\n",
      "[23] FHS, Hierarquia dos Diret√≥rios 96\n",
      "[24] Aprendendo Comandos do GNU/Linux 110\n",
      "[25] Localiza√ß√£o no sistema 120\n",
      "[26] T√≥picos para revis√£o do cap√≠tulo 127\n"
     ]
    }
   ],
   "source": [
    "chapters = []\n",
    "for c in toc_concepts:\n",
    "    if len(c.chapter) == 1:\n",
    "        chapters.append(c)\n",
    "        print(c.chapter, c.concept_name, c.page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac26919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Building Graph for 26 concepts...\n",
      "‚úÖ Graph saved: ebooks/LinuxFundamentals/initial_nodes.xml and ebooks/LinuxFundamentals/relations.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('ebooks/LinuxFundamentals/initial_nodes.xml'),\n",
       " PosixPath('ebooks/LinuxFundamentals/relations.xml'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StructuralGraphBuilder(base_output_dir)\n",
    "graph_builder.build_graph(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0fb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.pdf_conversor.generate_markdown(chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36f376",
   "metadata": {},
   "source": [
    "# Gerando grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c57770",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = base_output_dir\n",
    "OUTPUT_XML = ROOT_DIRECTORY / \"global_knowledge_graph.xml\"\n",
    "INITIAL_NODES = ROOT_DIRECTORY / 'initial_nodes.xml'\n",
    "NODES_XML = ROOT_DIRECTORY / 'nodes.xml'\n",
    "RELATIONS_XML = ROOT_DIRECTORY / 'relations.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9bac45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"The subject concept.\")\n",
    "    target: str = Field(description=\"The object concept.\")\n",
    "    relation_type: Literal['prerequisite', 'including', 'part-of', 'property', 'definition']\n",
    "    context: Optional[str] = Field(description=\"Justification text.\")\n",
    "\n",
    "class ConceptAnalysis(BaseModel):\n",
    "    \"\"\"LLM Output for a full chapter/concept file.\"\"\"\n",
    "    # We map 'new_concepts' to add to registry\n",
    "    new_concepts: List[str] = Field(description=\"List of MAIN concepts defined in this text.\")\n",
    "    relations: List[Relation] = Field(description=\"Semantic connections found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75b8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalRegistry:\n",
    "    def __init__(self):\n",
    "        # Stores simple strings: {\"Binary Notation\", \"Kernel\", \"File Permissions\"}\n",
    "        self.known_concepts = set()\n",
    "\n",
    "    def add_concepts(self, concepts: List[str]):\n",
    "        for c in concepts:\n",
    "            self.known_concepts.add(c)\n",
    "    \n",
    "    def get_context_string(self):\n",
    "        \"\"\"Returns a comma-separated string of known concepts for the prompt.\"\"\"\n",
    "        return \", \".join(sorted(list(self.known_concepts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e294535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_concept_content(current_node_name, text_content, registry, llm):\n",
    "    \"\"\"\n",
    "    Analyzes the entire markdown content for a specific concept node.\n",
    "    \"\"\"\n",
    "    if not text_content.strip():\n",
    "        return ConceptAnalysis(new_concepts=[], relations=[])\n",
    "\n",
    "    previous_concepts_str = registry.get_context_string()\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a Knowledge Graph Expert.\n",
    "        \n",
    "        Current Concept: '{current_node}'\n",
    "        \n",
    "        Your Goal:\n",
    "        1. List NEW concepts explicitly taught/defined here.\n",
    "        2. Extract Semantic Relations:\n",
    "           - **definition**: If '{current_node}' is defined here.\n",
    "           - **property**: Key attributes of '{current_node}'.\n",
    "           - **prerequisite**: Does this text require knowing a concept from the PREVIOUSLY LEARNED list?\n",
    "        \n",
    "        PREVIOUSLY LEARNED CONCEPTS:\n",
    "        [{history}]\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm.with_structured_output(ConceptAnalysis)\n",
    "    \n",
    "    try:\n",
    "        # We assume text_content fits in context window (usually fine for single concept sections)\n",
    "        return chain.invoke({\n",
    "            \"current_node\": current_node_name, \n",
    "            \"text\": text_content[:15000], # Safety cap for tokens\n",
    "            \"history\": previous_concepts_str\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è LLM Error: {e}\")\n",
    "        return ConceptAnalysis(new_concepts_taught=[], relations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e940986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_id(text):\n",
    "    \"\"\"Helper to create XML-safe IDs from concept names.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', text.strip())\n",
    "\n",
    "def prettify_xml(elem: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Return a pretty-printed XML string for the Element.\n",
    "    Strips the annoying extra newlines minidom likes to add.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    # Filter out lines that are purely whitespace\n",
    "    return '\\n'.join([line for line in reparsed.toprettyxml(indent=\"   \").split('\\n') if line.strip()])\n",
    "\n",
    "def process_book_sequentially():\n",
    "    print(\"üöÄ Starting Semantic Extraction Agent...\")\n",
    "    \n",
    "    # 1. SETUP\n",
    "    llm = ChatOpenAI(temperature=0, model=MODEL_NAME)\n",
    "    registry = GlobalRegistry() \n",
    "    \n",
    "    if not os.path.exists(INITIAL_NODES) or not os.path.exists(RELATIONS_XML):\n",
    "        print(\"‚ùå XML manifests not found. Run the structural builder first.\")\n",
    "        return\n",
    "\n",
    "    # 2. LOAD MANIFESTS\n",
    "    # We read nodes to know the order AND to append new nodes later\n",
    "    tree_nodes = ET.parse(INITIAL_NODES)\n",
    "    root_nodes = tree_nodes.getroot()\n",
    "    node_elements = root_nodes.findall(\"node\")\n",
    "    \n",
    "    # Sort strictly by order to respect book narrative\n",
    "    node_elements.sort(key=lambda x: int(x.get(\"order\", 0)))\n",
    "    \n",
    "    # Create a lookup of existing names to avoid duplicates when adding new ones\n",
    "    existing_node_names = {node.get(\"name\").lower().strip() for node in node_elements}\n",
    "    \n",
    "    # We read relations to APPEND to it\n",
    "    tree_rels = ET.parse(RELATIONS_XML)\n",
    "    root_rels = tree_rels.getroot()\n",
    "\n",
    "    print(f\"üìö Loaded {len(node_elements)} concepts in logical order.\")\n",
    "\n",
    "    # 3. PROCESSING LOOP\n",
    "    for node in node_elements:\n",
    "        node_id = node.get(\"id\")\n",
    "        node_name = node.get(\"name\")\n",
    "        folder_name = node.get(\"folder\")\n",
    "        \n",
    "        # Path to the granular MD file\n",
    "        file_path = os.path.join(ROOT_DIRECTORY, folder_name, \"document.md\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìñ Analyzing: {node_name}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # A. Semantic Analysis\n",
    "        analysis = analyze_concept_content(node_name, content, registry, llm)\n",
    "        \n",
    "        # B. Append New Relations to XML in memory\n",
    "        for sem_rel in analysis.relations:\n",
    "            rel_elem = ET.SubElement(root_rels, \"relation\")\n",
    "            rel_elem.set(\"type\", sem_rel.relation_type)\n",
    "            \n",
    "            if sem_rel.relation_type == 'prerequisite':\n",
    "                # Prerequisite Flow: The concept in history (Target of extraction) -> Current Node\n",
    "                rel_elem.set(\"source\", sem_rel.target) # The old concept\n",
    "                rel_elem.set(\"target\", node_id)                # The current concept\n",
    "            else:\n",
    "                # Definition/Property Flow: Current Node -> Attribute\n",
    "                rel_elem.set(\"source\", node_id)\n",
    "                rel_elem.set(\"target\", sem_rel.target)\n",
    "            \n",
    "            ET.SubElement(rel_elem, \"context\").text = sem_rel.context\n",
    "\n",
    "        # C. Update Registry (Learning) AND Nodes XML\n",
    "        if analysis.new_concepts:\n",
    "            new_count = 0\n",
    "            for concept in analysis.new_concepts:\n",
    "                clean_name = concept.strip()\n",
    "                if clean_name.lower() not in existing_node_names:\n",
    "                    # Create new Node entry\n",
    "                    new_node_id = f\"{sanitize_id(clean_name)}\"\n",
    "                    \n",
    "                    new_node = ET.SubElement(root_nodes, \"node\")\n",
    "                    new_node.set(\"id\", new_node_id)\n",
    "                    new_node.set(\"name\", clean_name)\n",
    "                    new_node.set(\"type\", \"extracted\")\n",
    "                    new_node.set(\"found_in_chapter\", node_id)\n",
    "                    \n",
    "                    existing_node_names.add(clean_name.lower())\n",
    "                    new_count += 1\n",
    "            \n",
    "            registry.add_concepts(analysis.new_concepts)\n",
    "            print(f\"üß† Learned: {len(analysis.new_concepts)} concepts ({new_count} new to XML)\")\n",
    "            \n",
    "            # Implicitly, the current node itself is now 'known'\n",
    "            registry.add_concepts([node_name])\n",
    "\n",
    "    # 4. SAVE FINAL XML FILES\n",
    "    # Save Relations\n",
    "    xml_rels_str = prettify_xml(root_rels)\n",
    "    with open(RELATIONS_XML, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(xml_rels_str)\n",
    "        \n",
    "    # Save Nodes\n",
    "    xml_nodes_str = prettify_xml(root_nodes)\n",
    "    with open(NODES_XML, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(xml_nodes_str)\n",
    "        \n",
    "    print(f\"\\n‚úÖ Updated {NODES_XML} and {RELATIONS_XML}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44d9499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Semantic Extraction Agent...\n",
      "üìö Loaded 26 concepts in logical order.\n",
      "\n",
      "üìñ Analyzing: Introdu√ß√£o ao Linux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:14:35,272 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 9 concepts (9 new to XML)\n",
      "\n",
      "üìñ Analyzing: CertiÔ¨Åca√ß√µes Linux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:14:41,532 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 5 concepts (5 new to XML)\n",
      "\n",
      "üìñ Analyzing: Hist√≥ria do Linux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:14:47,392 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 7 concepts (6 new to XML)\n",
      "\n",
      "üìñ Analyzing: Licen√ßas Open Source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:14:52,502 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 3 concepts (3 new to XML)\n",
      "\n",
      "üìñ Analyzing: Evolu√ß√£o do Linux: distribui√ß√µes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:14:59,306 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 11 concepts (11 new to XML)\n",
      "\n",
      "üìñ Analyzing: Conhecendo o Linux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:03,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (0 new to XML)\n",
      "\n",
      "üìñ Analyzing: T√≥picos para revis√£o do cap√≠tulo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:07,313 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 9 concepts (9 new to XML)\n",
      "\n",
      "üìñ Analyzing: Estrutura do sistema operacional\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:12,205 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 8 concepts (7 new to XML)\n",
      "\n",
      "üìñ Analyzing: O que √© um Shell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:17,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 7 concepts (7 new to XML)\n",
      "\n",
      "üìñ Analyzing: Vari√°veis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:25,081 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 5 concepts (4 new to XML)\n",
      "\n",
      "üìñ Analyzing: Arquivos de conÔ¨Ågura√ß√£o do shell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:33,849 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 8 concepts (8 new to XML)\n",
      "\n",
      "üìñ Analyzing: Caminhos de Diretorios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:38,436 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 3 concepts (3 new to XML)\n",
      "\n",
      "üìñ Analyzing: T√≥picos para revis√£o do cap√≠tulo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:43,351 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Analyzing: Como obter ajuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:44,865 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (1 new to XML)\n",
      "\n",
      "üìñ Analyzing: Formas de documenta√ß√£o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:49,627 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 4 concepts (3 new to XML)\n",
      "\n",
      "üìñ Analyzing: Comando help\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:15:52,758 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (0 new to XML)\n",
      "\n",
      "üìñ Analyzing: Comando apropos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:00,508 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (0 new to XML)\n",
      "\n",
      "üìñ Analyzing: Comando whatis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:03,703 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (0 new to XML)\n",
      "\n",
      "üìñ Analyzing: Comando man\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:06,887 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Analyzing: Comando info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:11,759 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 2 concepts (1 new to XML)\n",
      "\n",
      "üìñ Analyzing: Comando whereis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:16,358 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (0 new to XML)\n",
      "\n",
      "üìñ Analyzing: Comando which\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:18,054 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 1 concepts (0 new to XML)\n",
      "\n",
      "üìñ Analyzing: FHS, Hierarquia dos Diret√≥rios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:25,361 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Analyzing: Aprendendo Comandos do GNU/Linux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:29,786 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Learned: 9 concepts (9 new to XML)\n",
      "\n",
      "üìñ Analyzing: Localiza√ß√£o no sistema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:36,749 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Analyzing: T√≥picos para revis√£o do cap√≠tulo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 23:16:40,947 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Updated ebooks/LinuxFundamentals/nodes.xml and ebooks/LinuxFundamentals/relations.xml\n"
     ]
    }
   ],
   "source": [
    "process_book_sequentially()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e7a7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_HTML = base_output_dir / 'graph.html'\n",
    "OUTPUT_HTML = str(OUTPUT_HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ce0fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def create_interactive_graph():\n",
    "    if not os.path.exists(NODES_XML) or not os.path.exists(RELATIONS_XML):\n",
    "        print(\"‚ùå XML files not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Constructing NetworkX Graph...\")\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # 1. Parse Nodes\n",
    "    tree_nodes = ET.parse(NODES_XML)\n",
    "    for node in tree_nodes.findall(\"node\"):\n",
    "        node_id = node.get(\"id\")\n",
    "        name = node.get(\"name\")\n",
    "        node_type = node.get(\"type\", \"chapter\") \n",
    "        \n",
    "        # Obsidian Style: Dots with specific colors\n",
    "        color = \"#8bd3dd\"  # Cyan/Blue for Chapters\n",
    "        size = 15          # Standard size\n",
    "        title = f\"Type: {node_type}\"\n",
    "        \n",
    "        if node_type == \"extracted\":\n",
    "            color = \"#ffafcc\" # Pink/Pastel for Concepts\n",
    "            size = 10         # Smaller for concepts\n",
    "            found_in = node.get(\"found_in_chapter\", \"unknown\")\n",
    "            title += f\"\\nFound in: {found_in}\"\n",
    "        elif node_type == \"root\":\n",
    "            color = \"#f0a202\" # Gold for Root\n",
    "            size = 25\n",
    "\n",
    "        G.add_node(node_id, label=name, title=title, color=color, size=size, shape=\"dot\")\n",
    "\n",
    "    # 2. Parse Relations\n",
    "    tree_rels = ET.parse(RELATIONS_XML)\n",
    "    for rel in tree_rels.findall(\"relation\"):\n",
    "        source = rel.get(\"source\")\n",
    "        target = rel.get(\"target\")\n",
    "        rel_type = rel.get(\"type\")\n",
    "        \n",
    "        # Edges: Visibility Fix -> Brighter, Solid Colors\n",
    "        # color = \"#666666\" # Solid lighter gray for default edges\n",
    "        # width = 1\n",
    "        # dashes = False\n",
    "        color = \"#4a90e2\" # Solid Blue (instead of faint cyan)\n",
    "        dashes = True     # Keep dashes to distinguish structure\n",
    "        width = 2\n",
    "        \n",
    "        if rel_type == \"prerequisite\":\n",
    "          color = \"#ff4d6d\" # Bright Red/Pink\n",
    "          width = 3         # Thicker to stand out\n",
    "        # elif rel_type in [\"part-of\", \"including\"]:\n",
    "\n",
    "        # if G.has_node(source) and G.has_node(target):\n",
    "        G.add_edge(source, target, title=rel_type, color=color, width=width, dashes=dashes)\n",
    "\n",
    "    print(f\"üï∏Ô∏è  Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "    # 3. Generate PyVis Visualization (Obsidian Style)\n",
    "    print(\"üé® Generating Obsidian-like HTML Visualization...\")\n",
    "    \n",
    "    # Removed filter_menu and select_menu as requested\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#1e1e1e\", font_color=\"#cccccc\", select_menu=False, filter_menu=False)\n",
    "    \n",
    "    net.from_nx(G)\n",
    "    \n",
    "    # Physics & Interaction Options\n",
    "    # Removed transparency from shadow colors to improve crispness\n",
    "    options = \"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 0,\n",
    "        \"borderWidthSelected\": 2,\n",
    "        \"font\": {\n",
    "          \"size\": 14,\n",
    "          \"face\": \"tahoma\",\n",
    "          \"color\": \"#eeeeee\",\n",
    "          \"strokeWidth\": 2,\n",
    "          \"strokeColor\": \"#1e1e1e\"\n",
    "        },\n",
    "        \"shadow\": {\n",
    "            \"enabled\": true,\n",
    "            \"color\": \"black\",\n",
    "            \"size\": 5,\n",
    "            \"x\": 2,\n",
    "            \"y\": 2\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"smooth\": {\n",
    "          \"type\": \"continuous\",\n",
    "          \"forceDirection\": \"none\"\n",
    "        },\n",
    "        \"arrows\": {\n",
    "            \"to\": {\n",
    "                \"enabled\": true,\n",
    "                \"scaleFactor\": 0.5\n",
    "            }\n",
    "        },\n",
    "        \"color\": {\n",
    "            \"inherit\": false,\n",
    "            \"opacity\": 1.0\n",
    "        }\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"hoverConnectedEdges\": true,\n",
    "        \"selectConnectedEdges\": true,\n",
    "        \"navigationButtons\": true,\n",
    "        \"keyboard\": true,\n",
    "        \"tooltipDelay\": 200\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    net.set_options(options)\n",
    "    \n",
    "    # net.show_buttons(filter_=['physics'])\n",
    "\n",
    "    # Save\n",
    "    net.save_graph(OUTPUT_HTML)\n",
    "    print(f\"‚úÖ Visualization saved to: {os.path.abspath(OUTPUT_HTML)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921a971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Constructing NetworkX Graph...\n",
      "üï∏Ô∏è  Graph created with 280 nodes and 214 edges.\n",
      "üé® Generating Obsidian-like HTML Visualization...\n",
      "‚úÖ Visualization saved to: /home/pras/EMBRAPII/4linux/notebooks/ebooks/LinuxFundamentals/graph.html\n"
     ]
    }
   ],
   "source": [
    "create_interactive_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed654cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
